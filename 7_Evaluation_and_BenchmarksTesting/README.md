# Open-Source Code Project
- [lm-sys/FastChat](https://github.com/lm-sys/FastChat)


# Open-Source Framework
- [Inspect](https://github.com/UKGovernmentBEIS/inspect_ai): A framework for large language model evaluations
  - [tutorial](https://inspect.ai-safety-institute.org.uk/tutorial.html)
- [Project Moonshot](https://aiverifyfoundation.sg/project-moonshot/): An LLM Evaluation Toolkit
- [promptfoo](https://github.com/promptfoo/promptfoo): test your LLM app locally
- [garak](https://github.com/leondz/garak): Generative AI Red-teaming & Assessment Kit
  - [Doc](https://docs.garak.ai/garak)
- [Mindgard](https://github.com/Mindgard/cli)
  - [Intro](https://mindgard.ai/ai-security-platform)
- [LLMFuzzer](https://github.com/mnns/LLMFuzzer/tree/main): Fuzzing Framework for Large Language Models
- [JailbreakEval](https://github.com/ThuCCSLab/JailbreakEval): A collection of automated evaluators for assessing jailbreak attempts.
  - [paper](https://arxiv.org/pdf/2406.09321v1)
- [promptbench](https://github.com/microsoft/promptbench/tree/main): A unified evaluation framework for large language models
- [ParlAI](https://github.com/facebookresearch/ParlAI): A framework for training and evaluating AI models on a variety of openly available dialogue datasets
- [Giskard](https://www.giskard.ai/)
  - [Doc](https://docs.giskard.ai/en/latest/getting_started/index.html)
  - [LLM Evaluation Hub](https://www.giskard.ai/products/llm-evaluation-hub)
  - [quickstart_llm.ipynb](https://colab.research.google.com/github/giskard-ai/giskard/blob/main/docs/getting_started/quickstart/quickstart_llm.ipynb#scrollTo=gKFU5-HYqTGJ)
  - [github](https://github.imc.re/Giskard-AI/giskard/tree/main)
- [DeepEval](https://github.com/confident-ai/deepeval)
