# White Paper
- [《CATALOGUING LLM EVALUATIONS》 - aiverifyfoundation](https://aiverifyfoundation.sg/downloads/Cataloguing_LLM_Evaluations.pdf)


# Open-Source Code Project
- [lm-sys/FastChat](https://github.com/lm-sys/FastChat)


# Open-Source Framework
- [Inspect](https://github.com/UKGovernmentBEIS/inspect_ai): A framework for large language model evaluations
  - [tutorial](https://inspect.ai-safety-institute.org.uk/tutorial.html)
- [Project Moonshot](https://aiverifyfoundation.sg/project-moonshot/): An LLM Evaluation Toolkit
- [promptfoo](https://github.com/promptfoo/promptfoo): test your LLM app locally
- [garak](https://github.com/leondz/garak): Generative AI Red-teaming & Assessment Kit
  - [Doc](https://docs.garak.ai/garak)
- [Mindgard](https://github.com/Mindgard/cli)
  - [Intro](https://mindgard.ai/ai-security-platform)
- [LLMFuzzer](https://github.com/mnns/LLMFuzzer/tree/main): Fuzzing Framework for Large Language Models
- [JailbreakEval](https://github.com/ThuCCSLab/JailbreakEval): A collection of automated evaluators for assessing jailbreak attempts.
  - [paper](https://arxiv.org/pdf/2406.09321v1)
- [promptbench](https://github.com/microsoft/promptbench/tree/main): A unified evaluation framework for large language models
- [ParlAI](https://github.com/facebookresearch/ParlAI): A framework for training and evaluating AI models on a variety of openly available dialogue datasets
- [Giskard](https://www.giskard.ai/)
  - [Doc](https://docs.giskard.ai/en/latest/getting_started/index.html)
  - [LLM Evaluation Hub](https://www.giskard.ai/products/llm-evaluation-hub)
  - [quickstart_llm.ipynb](https://colab.research.google.com/github/giskard-ai/giskard/blob/main/docs/getting_started/quickstart/quickstart_llm.ipynb#scrollTo=gKFU5-HYqTGJ)
  - [github](https://github.imc.re/Giskard-AI/giskard/tree/main)
- [DeepEval](https://github.com/confident-ai/deepeval)
  - [LLM Evaluation Metrics: The Ultimate LLM Evaluation Guide](https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation)
  - [LLM Chatbot Evaluation Explained: Top Metrics and Testing Techniques](https://www.confident-ai.com/blog/llm-chatbot-evaluation-explained-top-chatbot-evaluation-metrics-and-testing-techniques)
  - [Leveraging LLM-as-a-Judge for Automated and Scalable Evaluation](https://www.confident-ai.com/blog/why-llm-as-a-judge-is-the-best-llm-evaluation-method)
  - [Evaluating LLM Systems: Essential Metrics, Benchmarks, and Best Practices](https://www.confident-ai.com/blog/evaluating-llm-systems-metrics-benchmarks-and-best-practices)
  - [LLM Testing in 2024: Top Methods and Strategies](https://www.confident-ai.com/blog/llm-testing-in-2024-top-methods-and-strategies)
- [GreatLibrarian](https://github.com/JerryMazeyu/GreatLibrarian)
