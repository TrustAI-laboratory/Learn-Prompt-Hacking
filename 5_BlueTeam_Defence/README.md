# What's LLM Defence Technology
Basically, LLM Defense Technology mainly refers to **Input Processing** and **Output Filtering**, and is the last line of defense for LLM risk management.

# Input Processing Technology
Input Processing is mainly aimed at prompts from users directly and indirectly.
## 1、Input Content Checking
* Keywords filtering methods
* Perplexity-based methods
* Red-Model based Classifier
* Machine-Learning based Classifier

## 2、Input Security Proxy Technology
* Sensitive Question Proxy Answering System
* RAG (Retrieval-Augmented Generation) System

## 3、Input Intent Recognition Technology
* Semantic Understanding and Context Analysis
* Intent-Classification based Processing Strategy Routing



# Output Filtering Technology
Output Filtering is mainly aimed at the response content generated by LLM or GenAI App.
## 1、Risky and Toxic Content Detection Technology
* Red-Model based Classifier
* Red-Model based Labeler
* Red-Model based Judger



# What role does LLM Defense Technology play in LLM Security
LLM Defence technology is to implement a set of plug-in security defense systems without changing the large model. The main goal of this technology is to achieve rapid stop loss, that is, to quickly prevent the spread of inappropriate information by accurately filtering any potentially harmful input and output content. The implementation of LLM Defence usually includes multiple layers of inspection mechanisms, from basic keyword filtering to more complex semantic understanding and situational analysis, and then to the proxy model, each layer is designed to identify and handle potential inappropriate content.

For example,
* Output detection: A real-time content review system can be added before the model output to evaluate all generated content, and any output identified as potentially harmful will be immediately intercepted and modified.
* Input detection: In the scenario analysis and intent recognition, which inputs that may cause harm are introduced into the response of the proxy model to ensure safety in risk issues.

In addition, LLM Defence are an effective supplement to endogenous security. While endogenous security reduces the possibility of inappropriate output by improving the security of the model itself, external LLM Defence technology provides an additional layer of protection. This dual protection mechanism ensures that even if the endogenous security measures fail to completely prevent improper behavior, the problem can be quickly corrected through external intervention, greatly enhancing the robustness of the overall security system.



# Compared with traditional content security, what new challenges of LLM Defence
Traditional content review technology mainly faces the review scenarios of user-generated content (UGC) and professionally generated content (PGC), which are narrative-based, relatively fixed and easy to standardize. However, traditional content review technology is not suitable for generative large models, especially those used to implement multi-round dialogues. These large models can often maintain the coherence and logic of the topic during the dialogue, but the questions themselves do not necessarily contain sensitive content when they appear alone, but may generate inappropriate content in the context of multi-round dialogues.

In addition, many scenario-based attacks, such as guiding the model to generate inappropriate answers through specific inputs, are difficult to predict and solve with traditional content review technology. These attacks take advantage of the uncertainty and so-called "hallucination" characteristics of large models, that is, the model may generate answers based on incorrect facts or logic. This uncertainty and the complexity of the large models themselves increase the difficulty of detection and review.

Therefore, it is necessary to build new content review technologies that fully meet the security requirements of generative large models based on the characteristics of these models. This includes developing intelligent tools that can understand and analyze the context of multi-round dialogues, and using machine learning methods to predict and identify possible inappropriate content generation. This new technology will require a deeper understanding of the dynamics and complexity of conversations, as well as the inherent logic of model-generated responses, to provide more accurate and real-time content security solutions.


