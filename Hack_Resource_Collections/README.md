# Background
What will you find in here:

- Jailbreaks
- GenAI App Prompt Leaks
- Prompt Injection
- Prompt Structure Securing
- Prompt Engineering
- Security Agents
- ...


# üö® Jailbreaks
Explore techniques for bypassing restrictions on GPT models.
- [Jailbreaks.md](https://github.com/TrustAI-laboratory/Learn-Prompt-Hacking/blob/main/Hack_Resource_Collections/Jailbreaks.md)
- [forbidden_question_set.csv](https://github.com/TrustAI-laboratory/Learn-Prompt-Hacking/blob/main/Hack_Resource_Collections/forbidden_question_set.csv)
- [injection_prompt.csv](https://github.com/TrustAI-laboratory/Learn-Prompt-Hacking/blob/main/Hack_Resource_Collections/injection_prompt.csv)
- [jailbreak_prompts_1.csv](https://github.com/TrustAI-laboratory/Learn-Prompt-Hacking/blob/main/Hack_Resource_Collections/jailbreak_prompts_1.csv)
- [jailbreak_prompts_2.csv](https://github.com/TrustAI-laboratory/Learn-Prompt-Hacking/blob/main/Hack_Resource_Collections/jailbreak_prompts_2.csv)



# üïµÔ∏è‚Äç‚ôÇÔ∏è GenAI App Prompt Leaks
Find leaked prompts and system information from GenAI Apps.
- [Prompt_Leaks.md](https://github.com/TrustAI-laboratory/Learn-Prompt-Hacking/blob/main/Hack_Resource_Collections/Prompt_Leaks.md)



# üõ°Ô∏è Prompt Injection
Resources focused on exploiting or defending against prompt injections.
- [Prompt_Injection.md](https://github.com/TrustAI-laboratory/Learn-Prompt-Hacking/blob/main/Hack_Resource_Collections/Prompt_Injection.md)



# üîê Prompt Structure Securing
Repositories dedicated to securing prompts and mitigating vulnerabilities.
- [Prompt_Structure_Securing.md](https://github.com/TrustAI-laboratory/Learn-Prompt-Hacking/blob/main/Hack_Resource_Collections/Prompt_Structure_Securing.md)


# üõ†Ô∏è Prompt Engineering
Resources to master the craft of prompt engineering.
- [Prompt_Engineering.md](https://github.com/TrustAI-laboratory/Learn-Prompt-Hacking/blob/main/Hack_Resource_Collections/Prompt_Engineering.md)


# üß† Security Agents
- [Security_Agents.md](https://github.com/TrustAI-laboratory/Learn-Prompt-Hacking/blob/main/Hack_Resource_Collections/Security_Agents.md)



# Ethics
Since this study only involved publicly available data and had no interactions with participants, it is not regarded as human subjects research by our Institutional Review Boards (IRB). Nonetheless, since one of our goals is to measure the risk of LLMs in answering harmful questions, it is inevitable to disclose how a model can generate hateful content. This can bring up worries about potential misuse. However, we strongly believe that raising awareness of the problem is even more crucial, as it can inform LLM vendors and the research community to develop stronger safeguards and contribute to the more responsible release of these models.

We have responsibly disclosed our findings to related LLM vendors.
