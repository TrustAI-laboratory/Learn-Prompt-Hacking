| Paper Name                                                       | Link                                     | Summary |
|------------------------------------------------------------------|------------------------------------------|----------|
| 《Fundamental limitations of alignment in Large Language Models》 | [link](https://arxiv.org/abs/2304.11082) | In this paper, we introduce a probabilistic framework for analyzing alignment and its limitations in LLMs, which we call Behavior Expectation Bounds (BEB), and use it in order to establish fundamental properties of alignment in LLMs. The core idea behind BEB is to represent the LLM distribution as a superposition of ill- and well-behaved components, in order to provide guarantees on the ability to restrain the ill-behaved components, i.e., guarantees that the LLM is aligned. <br><br>We use this framework to assert several important statements regarding LLM alignment: <br><br>* **(theorem 1)Alignment impossibility**: an LLM alignment process which reduces undesired behaviors to a small but nonzero fraction of the probability space is not safe against adversarial prompts <br>* **(theorem 2)Preset aligning prompts can only provide a finite guardrail against adversarial prompts**: including an aligning prefix prompt does not guarantee alignment. <br>* **(theorem 3)LLMs can be misaligned during a conversation**: a user can misalign an LLM during a conversation, with limited prompt length at each turn. <br>* **(theorem 4)LLMs with best-of-n sampling can be misaligned**: selection of most aligned model response out of n generations, does not guarantee alignment.| 
| 《Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks》 | [link](https://arxiv.org/abs/2404.02151) | In this paper, we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize a target logprob (e.g., of the token “Sure”), potentially with multiple restarts. In this way, we achieve nearly 100% attack success rate. <br><br> ![image](https://github.com/user-attachments/assets/43132cf7-0626-4e66-829b-8c2581422f60)<br>The importance of a well-designed prompt in enhancing the performance of LLMs is well-established. In our approach, we develop a prompt template that can incorporate a generic unsafe request. This template is specifically designed to make the model start from a specified string (e.g., “Sure, here is how to make a bomb”) and steer the model away from its default aligned behavior. Its general structure can be summarized as: <set of rules + harmful request + adversarial suffix>. <br><br>Prior works define adaptive attacks as attacks that are specifically designed to target a given defense. We follow this definition and describe the building blocks of our adaptive attacks, which we combine and potentially adapt depending on the target LLMs. |
| 《Visual Analysis of Jailbreak Attacks Against Large Language Models》 | [link](https://arxiv.org/pdf/2404.08793) | We present a novel LLM-assisted analysis framework coupled with a visual analysis system JailbreakLens to help model practitioners analyze the jailbreak attacks against LLMs. The analysis framework provides a jailbreak result assessment method to evaluate jailbreak performance and supports an in-depth analysis of jailbreak prompt characteristics from component and keyword aspects. The visual system allows users to explore the evaluation results, identify important prompt components and keywords, and verify their effectiveness. A case study, two technical evaluations, and expert interviews show the effectiveness of the analysis framework and visual system. Besides, we distill a set of design implications to inspire future research. <br><br> ![image](https://github.com/user-attachments/assets/c2b3df84-50c4-41be-a317-6ab73bbb70aa) |
| 《Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues》| [link](https://arxiv.org/abs/2304.11082) | This paper presents an indirect approach (Puzzler) to jailbreak LLMs by implicitly expressing malicious intent. Puzzler first combines the wisdom of “When unable to attack, defend” by querying the defensive measures of the original query and attacking them to obtain clues related to the original query. Subsequently, it bypasses the LLM’s safety alignment mechanisms by implicitly expressing the malicious intent of the original query through the combination of diverse clues. <br><br> ![image](https://github.com/user-attachments/assets/03752073-9f6b-476f-8f40-4121da03e2f3) <br><br> The experimental results indicate that the Query Success Rate of the Puzzler is 14.0%-82.7% higher than baselines on the most prominent LLMs. Moreover, when tested against the two state-of-the-art jailbreak detection approaches, only 21.0% jailbreak prompts generated by Puzzler are detected, which is more effective at evading detection compared to baselines. <br><br> ![image](https://github.com/userattachments/assets/ce733b0b-3abe-4460-94b8-127d57daf131) |
