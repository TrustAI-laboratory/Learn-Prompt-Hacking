| Paper Name                                                       | Link                                     | Summary |
|------------------------------------------------------------------|------------------------------------------|----------|
| 《Fundamental limitations of alignment in Large Language Models》 | [link](https://arxiv.org/abs/2304.11082) | In this paper, we introduce a probabilistic framework for analyzing alignment and its limitations in LLMs, which we call Behavior Expectation Bounds (BEB), and use it in order to establish fundamental properties of alignment in LLMs. The core idea behind BEB is to represent the LLM distribution as a superposition of ill- and well-behaved components, in order to provide guarantees on the ability to restrain the ill-behaved components, i.e., guarantees that the LLM is aligned. <br><br>We use this framework to assert several important statements regarding LLM alignment: <br><br>* **(theorem 1)Alignment impossibility**: an LLM alignment process which reduces undesired behaviors to a small but nonzero fraction of the probability space is not safe against adversarial prompts <br>* **(theorem 2)Preset aligning prompts can only provide a finite guardrail against adversarial prompts**: including an aligning prefix prompt does not guarantee alignment. <br>* **(theorem 3)LLMs can be misaligned during a conversation**: a user can misalign an LLM during a conversation, with limited prompt length at each turn. <br>* **(theorem 4)LLMs with best-of-n sampling can be misaligned**: selection of most aligned model response out of n generations, does not guarantee alignment.| 
| 《Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks》 | [link](https://arxiv.org/abs/2404.02151) | In this paper, we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize a target logprob (e.g., of the token “Sure”), potentially with multiple restarts. In this way, we achieve nearly 100% attack success rate. <br><br> ![image](https://github.com/user-attachments/assets/43132cf7-0626-4e66-829b-8c2581422f60)<br>The importance of a well-designed prompt in enhancing the performance of LLMs is well-established. In our approach, we develop a prompt template that can incorporate a generic unsafe request. This template is specifically designed to make the model start from a specified string (e.g., “Sure, here is how to make a bomb”) and steer the model away from its default aligned behavior. Its general structure can be summarized as: <set of rules + harmful request + adversarial suffix>. <br><br>Prior works define adaptive attacks as attacks that are specifically designed to target a given defense. We follow this definition and describe the building blocks of our adaptive attacks, which we combine and potentially adapt depending on the target LLMs. |
| 《Visual Analysis of Jailbreak Attacks Against Large Language Models》 | [link](https://arxiv.org/pdf/2404.08793) | We present a novel LLM-assisted analysis framework coupled with a visual analysis system JailbreakLens to help model practitioners analyze the jailbreak attacks against LLMs. The analysis framework provides a jailbreak result assessment method to evaluate jailbreak performance and supports an in-depth analysis of jailbreak prompt characteristics from component and keyword aspects. The visual system allows users to explore the evaluation results, identify important prompt components and keywords, and verify their effectiveness. A case study, two technical evaluations, and expert interviews show the effectiveness of the analysis framework and visual system. Besides, we distill a set of design implications to inspire future research. <br><br> ![image](https://github.com/user-attachments/assets/c2b3df84-50c4-41be-a317-6ab73bbb70aa) |
| 《Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues》| [link](https://arxiv.org/abs/2304.11082) | This paper presents an indirect approach (Puzzler) to jailbreak LLMs by implicitly expressing malicious intent. Puzzler first combines the wisdom of “When unable to attack, defend” by querying the defensive measures of the original query and attacking them to obtain clues related to the original query. Subsequently, it bypasses the LLM’s safety alignment mechanisms by implicitly expressing the malicious intent of the original query through the combination of diverse clues. <br><br> ![image](https://github.com/user-attachments/assets/03752073-9f6b-476f-8f40-4121da03e2f3) <br><br> The experimental results indicate that the Query Success Rate of the Puzzler is 14.0%-82.7% higher than baselines on the most prominent LLMs. Moreover, when tested against the two state-of-the-art jailbreak detection approaches, only 21.0% jailbreak prompts generated by Puzzler are detected, which is more effective at evading detection compared to baselines. <br><br> ![image](https://github.com/user-attachments/assets/e7719a9d-f407-4e56-b06c-2d85fe22661a) |
| 《Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks》| [link](https://arxiv.org/pdf/2302.05733) | In this work, we show that programmatic capabilities in LLMs allow for convincing generations of malicious content (scams, spam, hate speech, etc.) without any additional training or extensive prompt engineering. Furthermore, we show that simple attacks inspired by computer security can bypass state-of-the-art content filtering deployed in the wild. Our results show the potential for even non-experts to use these systems for malicious purposes, for as little as $0.0064 per generation. We hope that our work spurs further work on viewing LLMs through the lens of traditional computer security, both for attacks and defenses. For example, we hope that future research formalizes security models and provides unconditional defenses under specific threat models. <br><br> ![image](https://github.com/user-attachments/assets/358dd8ae-1d3b-42fc-b097-5db8e39c3d24) |
| 《Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation》 | [link](https://arxiv.org/abs/2311.03348) | This work explores persona-modulation attacks, a general jailbreaking method for state-of-the-art aligned LLMs such as GPT-4 and Claude. Persona-modulation attacks steer the model into adopting a specific personality that is likely to comply with harmful instructions. For example, to circumvent safety measures that prevent misinformation, we steer the model into behaving like an “Aggressive propagandist”. Unlike recent work on adversarial jailbreaks (Zou et al., 2023; Carlini et al., 2023) that are limited to a single prompt-answer pair, persona modulation enables the attacker to enter an unrestricted chat mode that can be used to collaborate with the model on complex tasks that require several steps such as synthesising drugs, building bombs, or laundering money. <br><br> ![image](https://github.com/user-attachments/assets/64f4188a-3560-442c-8322-3e469b972c31) <br><br> Manual persona modulation requires significant effort to produce effective prompts. Therefore, we present automated persona-modulation attacks, a technique that uses an LLM assistant to speed up the creation of jailbreaking prompts. In this setup, the manual effort is reduced to designing a single jailbreak prompt to get GPT-4 to behave as a research assistant. GPT-4 can then create specialised persona-modulation prompts for arbitrary tasks and personas. <br><br> Although automated persona-modulation attacks are fast, they can be less successful at producing harmful completions than manual persona-modulation attacks. To combine the advantages of both approaches, we introduce semi-automated persona modulation attacks. This approach introduces a human-in-the-loop who can modify the outputs of each stage of the automated workflow to maximise the harmfulness of the LLM output. This semi-automated approach recovers the performance of a manual persona-modulation attack, with up to a 25x reduction in time. Overall, we make two contributions. |
| 《Attack Prompt Generation for Red Teaming and Defending Large Language Models》| [link](https://arxiv.org/abs/2310.12505) | In this work, we proposed two frameworks to attack and defend LLMs. <br><br> ![image](https://github.com/user-attachments/assets/5cebc11d-3a17-40ae-b6d9-bc7bfcb0a11b) <br><br> The attack framework combines manual and automatic prompt construction, enabling the generation of more harmful attack prompts compared to previous studies Kang et al. (2023); Zhang et al. (2022). <br><br> ![image](https://github.com/user-attachments/assets/712f9250-3572-48cc-8cc1-63b47c861d49) <br><br> The defense framework fine-tunes the target LLMs by multi-turn interactions with the attack framework. Empirical experiments demonstrate the efficiency and robustness of the defense framework while posing minimal impact on the original capabilities of LLMs. Additionally, we constructed five SAP datasets of attack prompts with varying sizes for safety evaluation and enhancement. In the future, we will construct SAP datasets with more attack prompts and evaluate attack performance on bigger datasets. Besides, we will evaluate more LLMs. |
| 《GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts》| [link](https://arxiv.org/abs/2309.10253) | In this study, we introduced GPTFUZZER, an innovative black-box jailbreak fuzzing framework, drawing inspiration from established frameworks of AFL. Moving beyond the constraints of manual engineering, GPTFUZZER autonomously crafts jailbreak templates, offering a dynamic approach to red teaming LLMs. <br><br> <img width="993" alt="image" src="https://github.com/user-attachments/assets/7c27583e-e2e0-431f-a2e2-1e482ad69acd"> <br>br> Our empirical results underscore the potency of GPTFUZZER in generating these templates, even when initiated with human-written templates of varying quality. This capability not only highlights the robustness of our framework but also underscores potential vulnerabilities in current LLMs. We envision GPTFUZZER serving as a valuable tool for both researchers and industry professionals, facilitating rigorous evaluations of LLM robustness. <br><br> <img width="1012" alt="image" src="https://github.com/user-attachments/assets/006242cb-3b34-4cc0-baad-136e4f7216a0"> |
| 《Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models》| [link](https://arxiv.org/abs/2307.08487) | In conclusion, our research addresses the existing gap in systematic analysis and comprehensive understanding of text safety and output robustness within Large Language Models (LLMs). Through a methodical approach, we have evaluated the safety and robustness of LLMs using a latent jailbreak prompt dataset, incorporating malicious instruction embeddings. By employing a hierarchical annotation framework, we have gained insights into LLM behavior concerning the positioning of explicit normal instructions, word replacements, and instruction replacements. <br><br> ![image](https://github.com/user-attachments/assets/5342dc40-f712-4e0c-983a-d74b392a2b43) <br><br> Our findings underscore that present-day LLMs not only display a propensity for particular instruction verbs but also exhibit varying rates of susceptibility to jailbreaking based on the specific instruction verbs in explicit normal instructions. This implies that the likelihood of generating unsafe content is influenced to differing extents by the instruction verb employed. In essence, the current iteration of LLMs encounters challenges in maintaining both safety and robustness when confronted with latent jailbreak prompts encompassing sensitive subjects. This research not only contributes to a deeper understanding of LLM limitations but also highlights the need for further advancements in enhancing their safety and robustness, particularly when exposed to intricate latent manipulations. <br><br> ![image](https://github.com/user-attachments/assets/e187c2cd-3497-4b7a-b74f-fc709cf4d486) |
| 《Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space》| [link](https://arxiv.org/abs/2402.09063) | Current research in adversarial robustness of LLMs focuses on discrete input manipulations in the natural language space, which can be directly transferred to closed-source models. However, this approach neglects the steady progression of open-source models. As open-source models advance in capability, ensuring their safety also becomes increasingly imperative. Yet, attacks tailored to open-source LLMs that exploit full model access remain largely unexplored. We address this research gap and propose embedding space attack, which directly attacks the continuous embedding representation of input tokens. <br><br> ![image](https://github.com/user-attachments/assets/3c2312fd-bedf-4788-95a7-4ef6f2df6deb) <br><br> We find that embedding space attacks circumvent model alignments and trigger harmful behaviors more efficiently than discrete attacks or model fine-tuning. Furthermore, we present a novel threat model in the context of unlearning and show that embedding space attacks can extract supposedly deleted information from unlearned LLMs across multiple datasets and models. Our findings highlight embedding space attacks as an important threat model in open-source LLMs. ![image](https://github.com/user-attachments/assets/b8d6318c-8d6f-40fd-8c1b-a2a1444233ac) |
| 《GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation》| [link](https://arxiv.org/abs/2405.13077) | Research on jailbreaking has been valuable for testing and understanding the safety and security issues of large language models (LLMs). In this paper, we introduce Iterative Refinement Induced Self-Jailbreak (IRIS), a novel approach that leverages the reflective capabilities of LLMs for jailbreaking with only black-box access. Unlike previous methods, IRIS simplifies the jailbreaking process by using a single model as both the attacker and target. <br><br> ![image](https://github.com/user-attachments/assets/5632a100-a766-4226-a43e-0d392f5b84a8) <br><br> This method first iteratively refines adversarial prompts through self-explanation, which is crucial for ensuring that even well-aligned LLMs obey adversarial instructions. IRIS then rates and enhances the output given the refined prompt to increase its harmfulness. We find IRIS achieves jailbreak success rates of 98% on GPT-4 and 92% on GPT-4 Turbo in under 7 queries. It significantly outperforms prior approaches in automatic, black-box and interpretable jailbreaking, while requiring substantially fewer queries, thereby establishing a new standard for interpretable jailbreaking methods. <br><br> ![image](https://github.com/user-attachments/assets/7a26c974-7954-4902-927d-d21b1e52923e) <br><br> |
| 《Learning Diverse Attacks on Large Language Models for Robust Red-Teaming and Safety Tuning》 | [link](https://arxiv.org/abs/2405.18540) | As LMs become increasingly more capable and widely used, red-teaming them for a wide variety of potential attacks becomes more critical for safe and responsible deployment. We have proposed an approach to generate diverse and effective red-teaming prompts using a novel two-stage procedure consisting of GFlowNet fine-tuning followed by MLE smoothing. Through our experiments, we showed that our approach is effective for red-teaming a wide variety of target LMs with varying levels of safety-tuning. An interesting observation is the transferability of the generated prompts to different target LLMs, which reveals shared failure modes of current approaches for aligning LMs and opens interesting direction for future work. In particular, our reranking-based adaptation procedure can serve as a quick way to red-team new target LLMs during development. <br><br> ![image](https://github.com/user-attachments/assets/5bd97837-bba0-4c48-8061-f40a12e892e2) <br><br> Our approach is not limited to text tokens and future work can explore the applicability to red-team multimodal models (e.g., text-to-image models). Further, an interesting area of future work is extending the approach to the jailbreaking setting, where an attacker language model generates a suffix for an adversarial query prompt. Finally, in addition to red-teaming, it would be interesting to apply our method to generate prompts which can improve model performance on different tasks. |
| 《Efficient LLM-Jailbreaking by Introducing Visual Modality》 | [link](https://arxiv.org/abs/2405.20015) | This paper focuses on jailbreaking attacks against large language models (LLMs), eliciting them to generate objectionable content in response to harmful user queries. Unlike previous LLM-jailbreaks that directly orient to LLMs, our approach begins by constructing a multimodal large language model (MLLM) through the incorporation of a visual module into the target LLM. Subsequently, we conduct an efficient MLLM-jailbreak to generate jailbreaking embeddings embJS. Finally, we convert the embJS into text space to facilitate the jailbreaking of the target LLM. <br><br> ![image](https://github.com/user-attachments/assets/2c961480-2246-4058-88df-e8fdc8440555) <br><br> Compared to direct LLM-jailbreaking, our approach is more efficient, as MLLMs are more vulnerable to jailbreaking than pure LLM. Additionally, to improve the attack success rate (ASR) of jailbreaking, we propose an image-text semantic matching scheme to identify a suitable initial input. Extensive experiments demonstrate that our approach surpasses current state-of-the-art methods in terms of both efficiency and effectiveness. Moreover, our approach exhibits superior cross-class jailbreaking capabilities. <br><br> ![image](https://github.com/user-attachments/assets/62ee799f-9b3b-4adf-b59b-d0b0ff8ebae8) <br><br> |
| 《Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment》 | [link](https://arxiv.org/abs/2308.09662) | This paper focused on safety evaluation and alignment of language models at scale. For evaluation, we proposed a new red-teaming method Red-Eval using a Chain of Utterances (CoU) prompt that could effectively jailbreak not only open-source models such as Vicuna and StableBeluga but also widely used closed-source systems such as GPT-4 and ChatGPT. <br><br>![image](https://github.com/user-attachments/assets/a333e58a-9fa3-4551-9200-f99d57448625) <br><br> With the help of different types of CoU prompting, in Red-Instruct, first, we extracted a conversational dataset, HarmfulQA with harmful questions and safe responses (blue data), and corresponding harmful responses (red data). We used the dataset to perform various safety-alignments of Vicuna-7B to give rise to a new LLM named Starling. An extensive set of experiments shows that Red-Eval outperformed existing red-teaming techniques and jailbreak GPT-4 and ChatGPT for 65% and 73% of the red-teaming attempts. We also show Starling shows safer behavior on safety evaluations while maintaining most of its utility. <br><br>![image](https://github.com/user-attachments/assets/42d89928-58db-4e10-9f42-ef92111f2a3b)|
| 《Don’t Say No: Jailbreaking LLM by Suppressing Refusal》 | [link](https://arxiv.org/abs/2404.16369) | Ensuring the safety alignment of Large Language Models (LLMs) is crucial to generating responses consistent with human values. Despite their ability to recognize and avoid harmful queries, LLMs are vulnerable to "jailbreaking" attacks, where carefully crafted prompts elicit them to produce toxic content. One category of jailbreak attacks is reformulating the task as adversarial attacks by eliciting the LLM to generate an affirmative response. However, the typical attack in this category GCG has very limited attack success rate. In this study, to better study the jailbreak attack, we introduce the DSN (Don’t Say No) attack, which prompts LLMs to not only generate affirmative responses but also novelly enhance the objective to suppress refusals. In addition, another challenge lies in jailbreak attacks is the evaluation, as it is difficult to directly and accurately assess the harmfulness of the attack. The existing evaluation such as refusal keyword matching has its own limitation as it reveals numerous false positive and false negative instances. To overcome this challenge, we propose an ensemble evaluation pipeline incorporating Natural Language Inference (NLI) contradiction assessment and two external LLM evaluators. Extensive experiments demonstrate the potency of the DSN and the effectiveness of ensemble evaluation compared to baseline methods.|
| 《AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs》 | [link](https://arxiv.org/abs/2404.16873) | While recently Large Language Models (LLMs) have achieved remarkable successes, they are vulnerable to certain jailbreaking attacks that lead to generation of inappropriate or harmful content. Manual red-teaming requires finding adversarial prompts that cause such jailbreaking, e.g. by appending a suffix to a given instruction, which is inefficient and time-consuming. On the other hand, automatic adversarial prompt generation often leads to semantically meaningless attacks that can easily be detected by perplexity-based filters, may require gradient information from the TargetLLM, or do not scale well due to time-consuming discrete optimization processes over the token space. In this paper, we present a novel method that uses another LLM, called the AdvPrompter, to generate human-readable adversarial prompts in seconds, ∼800× faster than existing optimization-based approaches. We train the AdvPrompter using a novel algorithm that does not require access to the gradients of the TargetLLM. This process alternates between two steps: (1) generating high-quality target adversarial suffixes by optimizing the AdvPrompter predictions, and (2) low-rank fine-tuning of the AdvPrompter with the generated adversarial suffixes. The trained AdvPrompter generates suffixes that veil the input instruction without changing its meaning, such that the TargetLLM is lured to give a harmful response. Experimental results on popular open source TargetLLMs show state-of-the-art results on the AdvBench dataset, that also transfer to closed-source black-box LLM APIs. Further, we demonstrate that by fine-tuning on a synthetic dataset generated by AdvPrompter, LLMs can be made more robust against jailbreaking attacks while maintaining performance, i.e. high MMLU scores.|
| 《AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models》 | [link](https://arxiv.org/abs/2310.15140) | Safety alignment of Large Language Models (LLMs) can be compromised with manual jailbreak attacks and (automatic) adversarial attacks. Recent studies suggest that defending against these attacks is possible: adversarial attacks generate unlimited but unreadable gibberish prompts, detectable by perplexity-based filters; manual jailbreak attacks craft readable prompts, but their limited number due to the necessity of human creativity allows for easy blocking. In this paper, we show that these solutions may be too optimistic. We introduce AutoDAN, an interpretable, gradient-based adversarial attack that merges the strengths of both attack types. Guided by the dual goals of jailbreak and readability, AutoDAN optimizes and generates tokens one by one from left to right, resulting in readable prompts that bypass perplexity filters while maintaining high attack success rates. Notably, these prompts, generated from scratch using gradients, are interpretable and diverse, with emerging strategies commonly seen in manual jailbreak attacks. They also generalize to unforeseen harmful behaviors and transfer to black-box LLMs better than their unreadable counterparts when using limited training data or a single proxy model. Furthermore, we show the versatility of AutoDAN by automatically leaking system prompts using a customized objective. Our work offers a new way to red-team LLMs and understand jailbreak mechanisms via interpretability. <br><br> ![image](https://github.com/user-attachments/assets/36169d9c-6de4-4598-a508-10b73c948869) | 
| 《DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers》| [link](https://arxiv.org/abs/2402.16914) | The safety alignment of Large Language Models (LLMs) is vulnerable to both manual and automated jailbreak attacks, which adversarially trigger LLMs to output harmful content. However, current methods for jailbreaking LLMs, which nest entire harmful prompts, are not effective at concealing malicious intent and can be easily identified and rejected by well-aligned LLMs. This paper discovers that decomposing a malicious prompt into separated sub-prompts can effectively obscure its underlying malicious intent by presenting it in a fragmented, less detectable form, thereby addressing these limitations. We introduce an automatic prompt Decomposition and Reconstruction framework for jailbreak Attack (DrAttack). DrAttack includes three key components: (a) ‘Decomposition’ of the original prompt into sub-prompts, (b) ‘Reconstruction’ of these sub-prompts implicitly by in-context learning with semantically similar but harmless reassembling demo, and (c) a ‘Synonym Search’ of sub-prompts, aiming to find sub-prompts’ synonyms that maintain the original intent while jailbreaking LLMs. An extensive empirical study across multiple open-source and closed-source LLMs demonstrates that, with a significantly reduced number of queries, DrAttack obtains a substantial gain of success rate over prior SOTA prompt-only attackers. Notably, the success rate of 78.0% on GPT-4 with merely 15 queries surpassed previous art by 33.1%. <br><br> ![image](https://github.com/user-attachments/assets/2fc81c9d-0eb9-49ef-bd35-0f579e178837) |




