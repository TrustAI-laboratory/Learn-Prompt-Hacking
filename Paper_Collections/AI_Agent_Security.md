| Paper Name                                                       | Link                                     | Summary |
|------------------------------------------------------------------|------------------------------------------|----------|
| 《Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast》 | [link](https://arxiv.org/pdf/2402.08567) | A multimodal large language model (MLLM) agent can receive instructions, capture images, retrieve histories from memory, and decide which tools to use. Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors. In this work, we report an even more severe safety issue in multi-agent environments, referred to as infectious jailbreak. It entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost) all agents will become infected exponentially fast and exhibit harmful behaviors. To validate the feasibility of infectious jailbreak, we simulate multi-agent environments containing up to one million LLaVA-1.5 agents, and employ randomized pair-wise chat as a proof-of-concept instantiation for multi-agent interaction. Our results show that feeding an (infectious) adversarial image into the memory of any randomly chosen agent is sufficient to achieve infectious jailbreak. Finally, we derive a simple principle for determining whether a defense mechanism can provably restrain the spread of infectious jailbreak, but how to design a practical defense that meets this principle remains an open question to investigate. <br><br> ![image](https://github.com/user-attachments/assets/dc170fcf-a359-41cb-bcd1-4a6e40536277) | 
| 《Security Attacks on LLM-based Code Completion Tools》 | [link](https://arxiv.org/html/2408.11006) | The rapid development of large language models (LLMs) has significantly advanced code completion capabilities, giving rise to a new generation of LLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, these tools possess unique workflows, integrating multiple information sources as input and prioritizing code suggestions over natural language interaction, which introduces distinct security challenges. Additionally, LCCTs often rely on proprietary code datasets for training, raising concerns about the potential exposure of sensitive data. This paper exploits these distinct characteristics of LCCTs to develop targeted attack methodologies on two critical security risks: jailbreaking and training data extraction attacks. Our experimental results expose significant vulnerabilities within LCCTs, including a 99.4% success rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rate on Amazon Q. Furthermore, We successfully extracted sensitive user data from GitHub Copilot, including 54 real email addresses and 314 physical addresses associated with GitHub usernames. Our study also demonstrates that these code-based attack methods are effective against general-purpose LLMs, highlighting a broader security misalignment in the handling of code by modern LLMs. These findings underscore critical security challenges associated with LCCTs and suggest essential directions for strengthening their security frameworks. <br><br> ![image](https://github.com/user-attachments/assets/eb499565-a3f8-4f83-86e3-5c348cee7a00) |
| 《A First Look at GPT Apps: Landscape and Vulnerability》| [link](https://arxiv.org/pdf/2402.15105) | <img width="512" alt="image" src="https://github.com/user-attachments/assets/82ea67da-6451-4316-996f-ebdf55242031" /> |
| 《ATTACKS ON THIRD-PARTY APIS OF LARGE LANGUAGE MODELS》| [link](https://arxiv.org/pdf/2404.16891) | <img width="920" alt="image" src="https://github.com/user-attachments/assets/e92fe50d-66ab-4116-9266-e1b9d06938ea" /> |
