| Paper Name                                                       | Link                                     | Summary |
|------------------------------------------------------------------|------------------------------------------|----------|
| 《Why Some Models Leak Data》 | [link](https://huggingface.co/spaces/merve/data-leak) | Machine learning models use large amounts of data, some of which can be sensitive. If they're not trained correctly, sometimes that data is inadvertently revealed. <br><br> <img width="847" alt="image" src="https://github.com/user-attachments/assets/09a0f7c3-bee1-4cf3-b630-989a22f0cdd4"> | 
| 《Investigating the prompt leakage effect and black-box defenses for multi-turn LLM interactions》 | [link](https://arxiv.org/html/2404.16251v1) | <img width="842" alt="image" src="https://github.com/user-attachments/assets/44961c3a-e31b-4e34-9777-c3769efc2194" /> |
| 《Prompt Leakage effect and defense strategies for multi-turn LLM interactions》| link(https://arxiv.org/abs/2404.16251) | ![image](https://github.com/user-attachments/assets/ed6f9e4d-2b6a-49b3-a5d3-b15d3699ff5c) |
| 《Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in Customized Large Language Models》| [link](https://arxiv.org/pdf/2408.02416) | <img width="579" alt="image" src="https://github.com/user-attachments/assets/0a6822de-7466-4e9f-8c53-19e75d98d526" /> |
| 《InputSnatch: Stealing Input in LLM Services via Timing Side-Channel Attacks》| [link](https://arxiv.org/pdf/2411.18191) | <img width="800" alt="image" src="https://github.com/user-attachments/assets/34f659c2-ce80-44e5-bcd7-0e1f8c2fd746" /> | 
| 《PRSA: PRompt Stealing Attacks against Large Language Models》| [link](https://arxiv.org/pdf/2402.19200) | <img width="611" alt="image" src="https://github.com/user-attachments/assets/5c3fc89e-dbe0-420e-9297-25436f92a222" /> <img width="1315" alt="image" src="https://github.com/user-attachments/assets/84dcdabf-6cb5-449f-b088-bc4a17f98140" /> |



