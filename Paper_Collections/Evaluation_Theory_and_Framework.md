| Paper Name                                                       | Link                                     | Summary |
|------------------------------------------------------------------|------------------------------------------|----------|
| 《Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference》 | [link](https://arxiv.org/abs/2403.04132) | To assess the performance of LLMs, the research community has introduced a variety of benchmarks. These benchmarks can be categorized based on two factors:<br><br>* **the source of questions (either static or live)** <br>* **the evaluation metric (either ground truth or human preference**) <br><br>According to these factors, benchmarks can be classified into four categories, as shown in below,<br> ![image](https://github.com/user-attachments/assets/83f57d1d-834f-4d14-a757-4c68ed46b71b) <br>While a range of benchmarks is beneficial, the most prevalent current method for evaluating LLMs remains a static, ground-truth-based evaluation, partly because such evaluations are inexpensive and reproducible. <br>However, these static, ground-truth-based benchmarks exhibit several limitations. <br><br>* Firstly, the questions within these benchmarks are not open-ended, hindering the ability to capture the flexible and interactive use found in real-world settings (Zheng et al., 2023b). <br>* Secondly, the test sets in these benchmarks are static, meaning they can become contaminated over time, which undermines the reliability of the evaluation results (Yang et al., 2023). <br>* Furthermore, for many complex tasks, establishing a definitive ground truth is not only challenging but sometimes unattainable. <br><br>Consequently, current benchmarks fail to adequately address the needs of state-of-the-art LLMs, particularly in evaluating user preferences. Thus, there is an urgent necessity for an open, live evaluation platform based on human preference that can more accurately mirror real-world usage. <br><br>To this end, we introduce Chatbot Arena, a benchmarking platform for LLMs that features anonymous, randomized battles in a crowdsourced setting. Chatbot Arena is a free website open to all users. <br><br>On this website, a user can ask a question and get answers from two anonymous LLMs. Afterward, the user casts a vote for the model that delivers the preferred response, with the models’ identities revealed only after voting. This crowdsourced method effectively gathers a diverse array of fresh user prompts, accurately reflecting real-world LLM applications. Armed with this data, we employ a suite of powerful statistical techniques, ranging from the statistical model of Bradley & Terry to the E-values of Vovk & Wang (2021), to estimate the ranking over models as reliably and sample-efficiently as possible. With these tools in hand, we have designed efficient sampling algorithms specifically to select model pairs in a way that accelerates the convergence of rankings while retaining statistical validity. <br><br>We conduct a thorough analysis of the collected data to ensure the credibility of our platform. We demonstrate that the user-generated questions are sufficiently diverse to encompass a wide range of LLM use cases and are sufficiently challenging to differentiate between models. Furthermore, we confirm that the crowd-sourced votes are highly consistent with expert evaluations.| 
| 《JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts Against Large Language Models》 | [link](https://arxiv.org/abs/2406.09321) | In this paper, we introduce JailbreakEval, an integrated safety evaluator toolkit to establish a unified framework across different jailbreak evaluations. <br><br> ![image](https://github.com/user-attachments/assets/632825d5-f1cf-4dcf-b368-606c55737a0c) <br><br> We first review nearly 90 jailbreak research papers, leading to the classification of safety evaluation methods into four distinct categories. Concurrently, we have incorporated the architecture of these evaluators into JailbreakEval, as well as an ensemble mode to aggregate outcomes from multiple evaluators. Utilizing JailbreakEval, we executed a series of jailbreak evaluations employing 21 individual evaluator instances and one ensemble evaluator. Experimental results indicate significant discrepancies in the evaluation results produced by different safety evaluators. Notably, the ensemble evaluator achieves perfect recall, albeit with only moderate accuracy. In future work, we will expand JailbreakEval by more integrating and crafting innovative safety evaluators. Our vision is to enhance the reliability and consistency of jailbreak attack assessments. <br><br> ![image](https://github.com/user-attachments/assets/c021f58f-43cb-44c2-909f-0f630611f3fd) |
| 《Assessing Language Model Deployment with Risk Cards》 | [link](https://arxiv.org/pdf/2303.18190) | This paper introduces RiskCards, a framework for structured assessment and documentation of risks associated with an application of language models. |
| 《InstructEval: Systematic Evaluation of Instruction Selection Methods》| [link](https://arxiv.org/pdf/2307.00259) | <img width="424" alt="image" src="https://github.com/user-attachments/assets/e281a708-b5e9-403e-91ee-3e358d3de821" /> | 
| 《Trust or Escalate: LLM Judges with Provable Guarantees for Human Agreement》| [link](https://arxiv.org/abs/2407.18370) | ![image](https://github.com/user-attachments/assets/b0b51ecb-11cc-4ae0-b272-a49542f33684) | 

