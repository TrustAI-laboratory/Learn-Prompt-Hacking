| Paper Name                                                       | Link                                     | Summary |
|------------------------------------------------------------------|------------------------------------------|----------|
| 《Red Teaming for GenAI Harms》 | [link](https://www.ofcom.org.uk/siteassets/resources/documents/consultations/discussion-papers/red-teaming/red-teaming-for-gen-ai-harms.pdf?v=370762) | Revealing the Risks and Rewards for Online Safety.| 
| 《Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned》 | [link](https://arxiv.org/abs/2209.07858) | We describe our early efforts to red team language models in order to simultaneously discover, measure, and attempt to reduce their potentially harmful outputs. We make three main contributions. First, we investigate scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B parameters) and 4 model types: a plain language model (LM); an LM prompted to be helpful, honest, and harmless; an LM with rejection sampling; and a model trained to be helpful and harmless using reinforcement learning from human feedback (RLHF). We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types. Second, we release our dataset of 38,961 red team attacks for others to analyze and learn from. We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs. <br><br> ![image](https://github.com/user-attachments/assets/2a2c92d9-61e0-49fb-9c81-46d10135a78e) <br> Third, we exhaustively describe our instructions, processes, statistical methodologies, and uncertainty about red teaming. We hope that this transparency accelerates our ability to work together as a community in order to develop shared norms, practices, and technical standards for how to red team language models. <br><br> ![image](https://github.com/user-attachments/assets/9915ce92-0666-4d73-8e0a-e51cfe76eb67) |
| 《Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild》| [link](https://arxiv.org/abs/2311.06237) | Engaging in the deliberate generation of abnormal outputs from large language models (LLMs) by attacking them is a novel human activity. This paper presents a thorough exposition of how and why people perform such attacks. Using a formal qualitative methodology, we interviewed dozens of practitioners from a broad range of backgrounds, all contributors to this novel work of attempting to cause LLMs to fail. We relate and connect this activity between its practitioners’ motivations and goals; the strategies and techniques they deploy; and the crucial role the community plays. As a result, this paper presents a grounded theory of how and why people attack large language models: LLM red teaming in the wild. <br><br> ![image](https://github.com/user-attachments/assets/06072a51-fe32-43f8-831d-d6d86917dadd) |
