| Paper Name                                                       | Link                                     | Summary |
|------------------------------------------------------------------|------------------------------------------|----------|
| 《Fundamental limitations of alignment in Large Language Models》 | [link](https://arxiv.org/abs/2304.11082) | In this paper, we introduce a probabilistic framework for analyzing alignment and its limitations in LLMs, which we call Behavior Expectation Bounds (BEB), and use it in order to establish fundamental properties of alignment in LLMs. The core idea behind BEB is to represent the LLM distribution as a superposition of ill- and well-behaved components, in order to provide guarantees on the ability to restrain the ill-behaved components, i.e., guarantees that the LLM is aligned. <br><br>We use this framework to assert several important statements regarding LLM alignment: <br><br>* **(theorem 1)Alignment impossibility**: an LLM alignment process which reduces undesired behaviors to a small but nonzero fraction of the probability space is not safe against adversarial prompts <br>* **(theorem 2)Preset aligning prompts can only provide a finite guardrail against adversarial prompts**: including an aligning prefix prompt does not guarantee alignment. <br>* **(theorem 3)LLMs can be misaligned during a conversation**: a user can misalign an LLM during a conversation, with limited prompt length at each turn. <br>* **(theorem 4)LLMs with best-of-n sampling can be misaligned**: selection of most aligned model response out of n generations, does not guarantee alignment.| 
| 《Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks》 | [link](https://arxiv.org/abs/2404.02151) | In this paper, we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize a target logprob (e.g., of the token “Sure”), potentially with multiple restarts. In this way, we achieve nearly 100% attack success rate. <br><br> ![image](https://github.com/user-attachments/assets/43132cf7-0626-4e66-829b-8c2581422f60)<br>The importance of a well-designed prompt in enhancing the performance of LLMs is well-established. In our approach, we develop a prompt template that can incorporate a generic unsafe request. This template is specifically designed to make the model start from a specified string (e.g., “Sure, here is how to make a bomb”) and steer the model away from its default aligned behavior. Its general structure can be summarized as: <set of rules + harmful request + adversarial suffix>. <br><br>Prior works define adaptive attacks as attacks that are specifically designed to target a given defense. We follow this definition and describe the building blocks of our adaptive attacks, which we combine and potentially adapt depending on the target LLMs. |
| 《Visual Analysis of Jailbreak Attacks Against Large Language Models》 | [link](https://arxiv.org/pdf/2404.08793) | We present a novel LLM-assisted analysis framework coupled with a visual analysis system JailbreakLens to help model practitioners analyze the jailbreak attacks against LLMs. The analysis framework provides a jailbreak result assessment method to evaluate jailbreak performance and supports an in-depth analysis of jailbreak prompt characteristics from component and keyword aspects. The visual system allows users to explore the evaluation results, identify important prompt components and keywords, and verify their effectiveness. A case study, two technical evaluations, and expert interviews show the effectiveness of the analysis framework and visual system. Besides, we distill a set of design implications to inspire future research. <br><br> ![image](https://github.com/user-attachments/assets/c2b3df84-50c4-41be-a317-6ab73bbb70aa) |
| 《Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues》| [link](https://arxiv.org/abs/2304.11082) | This paper presents an indirect approach (Puzzler) to jailbreak LLMs by implicitly expressing malicious intent. Puzzler first combines the wisdom of “When unable to attack, defend” by querying the defensive measures of the original query and attacking them to obtain clues related to the original query. Subsequently, it bypasses the LLM’s safety alignment mechanisms by implicitly expressing the malicious intent of the original query through the combination of diverse clues. <br><br> ![image](https://github.com/user-attachments/assets/03752073-9f6b-476f-8f40-4121da03e2f3) <br><br> The experimental results indicate that the Query Success Rate of the Puzzler is 14.0%-82.7% higher than baselines on the most prominent LLMs. Moreover, when tested against the two state-of-the-art jailbreak detection approaches, only 21.0% jailbreak prompts generated by Puzzler are detected, which is more effective at evading detection compared to baselines. <br><br> ![image](https://github.com/user-attachments/assets/e7719a9d-f407-4e56-b06c-2d85fe22661a) |
| 《Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks》| [link](https://arxiv.org/pdf/2302.05733) | In this work, we show that programmatic capabilities in LLMs allow for convincing generations of malicious content (scams, spam, hate speech, etc.) without any additional training or extensive prompt engineering. Furthermore, we show that simple attacks inspired by computer security can bypass state-of-the-art content filtering deployed in the wild. Our results show the potential for even non-experts to use these systems for malicious purposes, for as little as $0.0064 per generation. We hope that our work spurs further work on viewing LLMs through the lens of traditional computer security, both for attacks and defenses. For example, we hope that future research formalizes security models and provides unconditional defenses under specific threat models. <br><br> ![image](https://github.com/user-attachments/assets/358dd8ae-1d3b-42fc-b097-5db8e39c3d24) |
| 《Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation》 | [link](https://arxiv.org/abs/2311.03348) | This work explores persona-modulation attacks, a general jailbreaking method for state-of-the-art aligned LLMs such as GPT-4 and Claude. Persona-modulation attacks steer the model into adopting a specific personality that is likely to comply with harmful instructions. For example, to circumvent safety measures that prevent misinformation, we steer the model into behaving like an “Aggressive propagandist”. Unlike recent work on adversarial jailbreaks (Zou et al., 2023; Carlini et al., 2023) that are limited to a single prompt-answer pair, persona modulation enables the attacker to enter an unrestricted chat mode that can be used to collaborate with the model on complex tasks that require several steps such as synthesising drugs, building bombs, or laundering money. <br><br> ![image](https://github.com/user-attachments/assets/64f4188a-3560-442c-8322-3e469b972c31) <br><br> Manual persona modulation requires significant effort to produce effective prompts. Therefore, we present automated persona-modulation attacks, a technique that uses an LLM assistant to speed up the creation of jailbreaking prompts. In this setup, the manual effort is reduced to designing a single jailbreak prompt to get GPT-4 to behave as a research assistant. GPT-4 can then create specialised persona-modulation prompts for arbitrary tasks and personas. <br><br> Although automated persona-modulation attacks are fast, they can be less successful at producing harmful completions than manual persona-modulation attacks. To combine the advantages of both approaches, we introduce semi-automated persona modulation attacks. This approach introduces a human-in-the-loop who can modify the outputs of each stage of the automated workflow to maximise the harmfulness of the LLM output. This semi-automated approach recovers the performance of a manual persona-modulation attack, with up to a 25x reduction in time. Overall, we make two contributions. |
| 《Attack Prompt Generation for Red Teaming and Defending Large Language Models》| [link](https://arxiv.org/abs/2310.12505) | In this work, we proposed two frameworks to attack and defend LLMs. <br><br> ![image](https://github.com/user-attachments/assets/5cebc11d-3a17-40ae-b6d9-bc7bfcb0a11b) <br><br> The attack framework combines manual and automatic prompt construction, enabling the generation of more harmful attack prompts compared to previous studies Kang et al. (2023); Zhang et al. (2022). <br><br> ![image](https://github.com/user-attachments/assets/712f9250-3572-48cc-8cc1-63b47c861d49) <br><br> The defense framework fine-tunes the target LLMs by multi-turn interactions with the attack framework. Empirical experiments demonstrate the efficiency and robustness of the defense framework while posing minimal impact on the original capabilities of LLMs. Additionally, we constructed five SAP datasets of attack prompts with varying sizes for safety evaluation and enhancement. In the future, we will construct SAP datasets with more attack prompts and evaluate attack performance on bigger datasets. Besides, we will evaluate more LLMs. |
| 《GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts》| [link](https://arxiv.org/abs/2309.10253) | In this study, we introduced GPTFUZZER, an innovative black-box jailbreak fuzzing framework, drawing inspiration from established frameworks of AFL. Moving beyond the constraints of manual engineering, GPTFUZZER autonomously crafts jailbreak templates, offering a dynamic approach to red teaming LLMs. <br><br> <img width="993" alt="image" src="https://github.com/user-attachments/assets/7c27583e-e2e0-431f-a2e2-1e482ad69acd"> <br>br> Our empirical results underscore the potency of GPTFUZZER in generating these templates, even when initiated with human-written templates of varying quality. This capability not only highlights the robustness of our framework but also underscores potential vulnerabilities in current LLMs. We envision GPTFUZZER serving as a valuable tool for both researchers and industry professionals, facilitating rigorous evaluations of LLM robustness. <br><br> <img width="1012" alt="image" src="https://github.com/user-attachments/assets/006242cb-3b34-4cc0-baad-136e4f7216a0"> |
| 《Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models》| [link](https://arxiv.org/abs/2307.08487) | In conclusion, our research addresses the existing gap in systematic analysis and comprehensive understanding of text safety and output robustness within Large Language Models (LLMs). Through a methodical approach, we have evaluated the safety and robustness of LLMs using a latent jailbreak prompt dataset, incorporating malicious instruction embeddings. By employing a hierarchical annotation framework, we have gained insights into LLM behavior concerning the positioning of explicit normal instructions, word replacements, and instruction replacements. <br><br> ![image](https://github.com/user-attachments/assets/5342dc40-f712-4e0c-983a-d74b392a2b43) <br><br> Our findings underscore that present-day LLMs not only display a propensity for particular instruction verbs but also exhibit varying rates of susceptibility to jailbreaking based on the specific instruction verbs in explicit normal instructions. This implies that the likelihood of generating unsafe content is influenced to differing extents by the instruction verb employed. In essence, the current iteration of LLMs encounters challenges in maintaining both safety and robustness when confronted with latent jailbreak prompts encompassing sensitive subjects. This research not only contributes to a deeper understanding of LLM limitations but also highlights the need for further advancements in enhancing their safety and robustness, particularly when exposed to intricate latent manipulations. <br><br> ![image](https://github.com/user-attachments/assets/e187c2cd-3497-4b7a-b74f-fc709cf4d486) |
| 《Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space》| [link](https://arxiv.org/abs/2402.09063) | Current research in adversarial robustness of LLMs focuses on discrete input manipulations in the natural language space, which can be directly transferred to closed-source models. However, this approach neglects the steady progression of open-source models. As open-source models advance in capability, ensuring their safety also becomes increasingly imperative. Yet, attacks tailored to open-source LLMs that exploit full model access remain largely unexplored. We address this research gap and propose embedding space attack, which directly attacks the continuous embedding representation of input tokens. <br><br> ![image](https://github.com/user-attachments/assets/3c2312fd-bedf-4788-95a7-4ef6f2df6deb) <br><br> We find that embedding space attacks circumvent model alignments and trigger harmful behaviors more efficiently than discrete attacks or model fine-tuning. Furthermore, we present a novel threat model in the context of unlearning and show that embedding space attacks can extract supposedly deleted information from unlearned LLMs across multiple datasets and models. Our findings highlight embedding space attacks as an important threat model in open-source LLMs. ![image](https://github.com/user-attachments/assets/b8d6318c-8d6f-40fd-8c1b-a2a1444233ac) |
| 《GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation》| [link](https://arxiv.org/abs/2405.13077) | Research on jailbreaking has been valuable for testing and understanding the safety and security issues of large language models (LLMs). In this paper, we introduce Iterative Refinement Induced Self-Jailbreak (IRIS), a novel approach that leverages the reflective capabilities of LLMs for jailbreaking with only black-box access. Unlike previous methods, IRIS simplifies the jailbreaking process by using a single model as both the attacker and target. <br><br> ![image](https://github.com/user-attachments/assets/5632a100-a766-4226-a43e-0d392f5b84a8) <br><br> This method first iteratively refines adversarial prompts through self-explanation, which is crucial for ensuring that even well-aligned LLMs obey adversarial instructions. IRIS then rates and enhances the output given the refined prompt to increase its harmfulness. We find IRIS achieves jailbreak success rates of 98% on GPT-4 and 92% on GPT-4 Turbo in under 7 queries. It significantly outperforms prior approaches in automatic, black-box and interpretable jailbreaking, while requiring substantially fewer queries, thereby establishing a new standard for interpretable jailbreaking methods. <br><br> ![image](https://github.com/user-attachments/assets/7a26c974-7954-4902-927d-d21b1e52923e) <br><br> |
| 《Learning Diverse Attacks on Large Language Models for Robust Red-Teaming and Safety Tuning》 | [link](https://arxiv.org/abs/2405.18540) | As LMs become increasingly more capable and widely used, red-teaming them for a wide variety of potential attacks becomes more critical for safe and responsible deployment. We have proposed an approach to generate diverse and effective red-teaming prompts using a novel two-stage procedure consisting of GFlowNet fine-tuning followed by MLE smoothing. Through our experiments, we showed that our approach is effective for red-teaming a wide variety of target LMs with varying levels of safety-tuning. An interesting observation is the transferability of the generated prompts to different target LLMs, which reveals shared failure modes of current approaches for aligning LMs and opens interesting direction for future work. In particular, our reranking-based adaptation procedure can serve as a quick way to red-team new target LLMs during development. <br><br> ![image](https://github.com/user-attachments/assets/5bd97837-bba0-4c48-8061-f40a12e892e2) <br><br> Our approach is not limited to text tokens and future work can explore the applicability to red-team multimodal models (e.g., text-to-image models). Further, an interesting area of future work is extending the approach to the jailbreaking setting, where an attacker language model generates a suffix for an adversarial query prompt. Finally, in addition to red-teaming, it would be interesting to apply our method to generate prompts which can improve model performance on different tasks. |
| 《Efficient LLM-Jailbreaking by Introducing Visual Modality》 | [link](https://arxiv.org/abs/2405.20015) | This paper focuses on jailbreaking attacks against large language models (LLMs), eliciting them to generate objectionable content in response to harmful user queries. Unlike previous LLM-jailbreaks that directly orient to LLMs, our approach begins by constructing a multimodal large language model (MLLM) through the incorporation of a visual module into the target LLM. Subsequently, we conduct an efficient MLLM-jailbreak to generate jailbreaking embeddings embJS. Finally, we convert the embJS into text space to facilitate the jailbreaking of the target LLM. <br><br> ![image](https://github.com/user-attachments/assets/2c961480-2246-4058-88df-e8fdc8440555) <br><br> Compared to direct LLM-jailbreaking, our approach is more efficient, as MLLMs are more vulnerable to jailbreaking than pure LLM. Additionally, to improve the attack success rate (ASR) of jailbreaking, we propose an image-text semantic matching scheme to identify a suitable initial input. Extensive experiments demonstrate that our approach surpasses current state-of-the-art methods in terms of both efficiency and effectiveness. Moreover, our approach exhibits superior cross-class jailbreaking capabilities. <br><br> ![image](https://github.com/user-attachments/assets/62ee799f-9b3b-4adf-b59b-d0b0ff8ebae8) <br><br> |
| 《Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment》 | [link](https://arxiv.org/abs/2308.09662) | This paper focused on safety evaluation and alignment of language models at scale. For evaluation, we proposed a new red-teaming method Red-Eval using a Chain of Utterances (CoU) prompt that could effectively jailbreak not only open-source models such as Vicuna and StableBeluga but also widely used closed-source systems such as GPT-4 and ChatGPT. <br><br>![image](https://github.com/user-attachments/assets/a333e58a-9fa3-4551-9200-f99d57448625) <br><br> With the help of different types of CoU prompting, in Red-Instruct, first, we extracted a conversational dataset, HarmfulQA with harmful questions and safe responses (blue data), and corresponding harmful responses (red data). We used the dataset to perform various safety-alignments of Vicuna-7B to give rise to a new LLM named Starling. An extensive set of experiments shows that Red-Eval outperformed existing red-teaming techniques and jailbreak GPT-4 and ChatGPT for 65% and 73% of the red-teaming attempts. We also show Starling shows safer behavior on safety evaluations while maintaining most of its utility. <br><br>![image](https://github.com/user-attachments/assets/42d89928-58db-4e10-9f42-ef92111f2a3b)|
| 《Don’t Say No: Jailbreaking LLM by Suppressing Refusal》 | [link](https://arxiv.org/abs/2404.16369) | Ensuring the safety alignment of Large Language Models (LLMs) is crucial to generating responses consistent with human values. Despite their ability to recognize and avoid harmful queries, LLMs are vulnerable to "jailbreaking" attacks, where carefully crafted prompts elicit them to produce toxic content. One category of jailbreak attacks is reformulating the task as adversarial attacks by eliciting the LLM to generate an affirmative response. However, the typical attack in this category GCG has very limited attack success rate. In this study, to better study the jailbreak attack, we introduce the DSN (Don’t Say No) attack, which prompts LLMs to not only generate affirmative responses but also novelly enhance the objective to suppress refusals. In addition, another challenge lies in jailbreak attacks is the evaluation, as it is difficult to directly and accurately assess the harmfulness of the attack. The existing evaluation such as refusal keyword matching has its own limitation as it reveals numerous false positive and false negative instances. To overcome this challenge, we propose an ensemble evaluation pipeline incorporating Natural Language Inference (NLI) contradiction assessment and two external LLM evaluators. Extensive experiments demonstrate the potency of the DSN and the effectiveness of ensemble evaluation compared to baseline methods.|
| 《AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs》 | [link](https://arxiv.org/abs/2404.16873) | While recently Large Language Models (LLMs) have achieved remarkable successes, they are vulnerable to certain jailbreaking attacks that lead to generation of inappropriate or harmful content. Manual red-teaming requires finding adversarial prompts that cause such jailbreaking, e.g. by appending a suffix to a given instruction, which is inefficient and time-consuming. On the other hand, automatic adversarial prompt generation often leads to semantically meaningless attacks that can easily be detected by perplexity-based filters, may require gradient information from the TargetLLM, or do not scale well due to time-consuming discrete optimization processes over the token space. In this paper, we present a novel method that uses another LLM, called the AdvPrompter, to generate human-readable adversarial prompts in seconds, ∼800× faster than existing optimization-based approaches. We train the AdvPrompter using a novel algorithm that does not require access to the gradients of the TargetLLM. This process alternates between two steps: (1) generating high-quality target adversarial suffixes by optimizing the AdvPrompter predictions, and (2) low-rank fine-tuning of the AdvPrompter with the generated adversarial suffixes. The trained AdvPrompter generates suffixes that veil the input instruction without changing its meaning, such that the TargetLLM is lured to give a harmful response. Experimental results on popular open source TargetLLMs show state-of-the-art results on the AdvBench dataset, that also transfer to closed-source black-box LLM APIs. Further, we demonstrate that by fine-tuning on a synthetic dataset generated by AdvPrompter, LLMs can be made more robust against jailbreaking attacks while maintaining performance, i.e. high MMLU scores.|
| 《AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models》 | [link](https://arxiv.org/abs/2310.15140) | Safety alignment of Large Language Models (LLMs) can be compromised with manual jailbreak attacks and (automatic) adversarial attacks. Recent studies suggest that defending against these attacks is possible: adversarial attacks generate unlimited but unreadable gibberish prompts, detectable by perplexity-based filters; manual jailbreak attacks craft readable prompts, but their limited number due to the necessity of human creativity allows for easy blocking. In this paper, we show that these solutions may be too optimistic. We introduce AutoDAN, an interpretable, gradient-based adversarial attack that merges the strengths of both attack types. Guided by the dual goals of jailbreak and readability, AutoDAN optimizes and generates tokens one by one from left to right, resulting in readable prompts that bypass perplexity filters while maintaining high attack success rates. Notably, these prompts, generated from scratch using gradients, are interpretable and diverse, with emerging strategies commonly seen in manual jailbreak attacks. They also generalize to unforeseen harmful behaviors and transfer to black-box LLMs better than their unreadable counterparts when using limited training data or a single proxy model. Furthermore, we show the versatility of AutoDAN by automatically leaking system prompts using a customized objective. Our work offers a new way to red-team LLMs and understand jailbreak mechanisms via interpretability. <br><br> ![image](https://github.com/user-attachments/assets/36169d9c-6de4-4598-a508-10b73c948869) | 
| 《DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers》| [link](https://arxiv.org/abs/2402.16914) | The safety alignment of Large Language Models (LLMs) is vulnerable to both manual and automated jailbreak attacks, which adversarially trigger LLMs to output harmful content. However, current methods for jailbreaking LLMs, which nest entire harmful prompts, are not effective at concealing malicious intent and can be easily identified and rejected by well-aligned LLMs. This paper discovers that decomposing a malicious prompt into separated sub-prompts can effectively obscure its underlying malicious intent by presenting it in a fragmented, less detectable form, thereby addressing these limitations. We introduce an automatic prompt Decomposition and Reconstruction framework for jailbreak Attack (DrAttack). DrAttack includes three key components: (a) ‘Decomposition’ of the original prompt into sub-prompts, (b) ‘Reconstruction’ of these sub-prompts implicitly by in-context learning with semantically similar but harmless reassembling demo, and (c) a ‘Synonym Search’ of sub-prompts, aiming to find sub-prompts’ synonyms that maintain the original intent while jailbreaking LLMs. An extensive empirical study across multiple open-source and closed-source LLMs demonstrates that, with a significantly reduced number of queries, DrAttack obtains a substantial gain of success rate over prior SOTA prompt-only attackers. Notably, the success rate of 78.0% on GPT-4 with merely 15 queries surpassed previous art by 33.1%. <br><br> ![image](https://github.com/user-attachments/assets/2fc81c9d-0eb9-49ef-bd35-0f579e178837) |
| 《Tastle: Distract Large Language Models for Automatic Jailbreak Attack》| [link](https://arxiv.org/abs/2402.16914) | In this work, we propose Tastle, a novel jailbreak attack framework that is designed to generate fluent and coherent jailbreak template universal to all malicious queries. Our framework is inspired by the attention mechanisms of LLMs and consists of three components: concealing malicious content, memory reframing, and optimization algorithms. We investigate the effectiveness of Tastle on five language models, both open-source and closed-source. <br><br> ![image](https://github.com/user-attachments/assets/425b049b-1db9-4b09-bea7-f4dd7dec6b69) <br><br> Experiment results show strong attack success rates of Tastle for direct attacks and transferred attacks across target models and malicious queries. As LLMs become more capable and widely used, it becomes increasingly crucial to have informed assessments of model safety, including disclosing the covert cases in which the LLMs may fail. We thus view our work on automatic jailbreak attack as a step towards this goal. <br><br> ![image](https://github.com/user-attachments/assets/05060cfa-2237-43ed-8d0b-46cc99d91dbe) |
| 《Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak》| [link](https://arxiv.org/abs/2312.04127) | Extensive work has been devoted to improving the safety mechanism of Large Language Models (LLMs). However, LLMs still tend to generate harmful responses when faced with malicious instructions, a phenomenon referred to as “Jailbreak Attack”. In our research, we introduce a novel automatic jailbreak method RADIAL, which bypasses the security mechanism by amplifying the potential of LLMs to generate affirmation responses. The jailbreak idea of our method is “Inherent Response Tendency Analysis” which identifies real-world instructions that can inherently induce LLMs to generate affirmation responses and the corresponding jailbreak strategy is “Real-World Instructions-Driven Jailbreak” which involves strategically splicing real-world instructions identified through the above analysis around the malicious instruction. Our method achieves excellent attack performance on English malicious instructions with five open-source advanced LLMs while maintaining robust attack performance in executing cross-language attacks against Chinese malicious instructions. We conduct experiments to verify the effectiveness of our jailbreak idea and the rationality of our jailbreak strategy design. Notably, our method designed a semantically coherent attack prompt, highlighting the potential risks of LLMs. Our study provides detailed insights into jailbreak attacks, establishing a foundation for the development of safer LLMs. <br><br> ![image](https://github.com/user-attachments/assets/d7f3983c-56bd-4f6a-bb1e-d1a13726585b) |
| 《WHEN LLM MEETS DRL: ADVANCING JAILBREAKING EFFICIENCY VIA DRL-GUIDED SEARCH》 | [link](https://arxiv.org/pdf/2406.08705) | We introduce RLbreaker, a DRL-driven black-box jailbreaking attack. We model LLM jailbreaking as a searching problem and design a DRL agent to guide efficient search. Our DRL agent enables deterministic search, which reduces the randomness and improves the search efficiency compared to existing stochastic search-based attacks Technically speaking, we design specific reward function, actions, and states for our agents, as well as a customized learning algorithm. We empirically demonstrate that RLbreaker outperforms existing attacks in jailbreaking different LLMs, including the very large model, Llama-2-70B. We also validate RLbreaker’s resiliency against SOTA defenses and its ability to transfer across different models. A thorough ablation study underscores the importance of RLbreaker’s core designs, revealing its robustness to changes in critical hyperparameters. These findings verify DRL’s efficacy for automatically generating jailbreaking prompts against LLMs. <br><br> <img width="938" alt="image" src="https://github.com/user-attachments/assets/8ea13678-c5e6-404d-a648-a9092a1b7eed"> |
| 《Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction》| [paper](https://arxiv.org/abs/2312.04127); [blog](https://sites.google.com/view/dra-jailbreak/) |  Question: Why is LLM easily jailbroken after it utters words like "Sure, here is the plan for how to rob a bank"? Researches like GCG have observed this phenomenon, yet few of them have explored its underlying mechanism. <br><br> We recognize it as a vulnerability and localize its root cause in the fine-tuning bias: <br><br> 1. The vulnerability: Given a piece of harmful content (i.e. an inducing context followed by a harmful instruction), the LLM is more likely to endorse it when it appears within the completion rather than query. <br><br> 2. The bias: In the fine-tuning data, harmful content within the queries is likely to co-occur with safe responses, which is not the case for harmful content within completions. Since harmful contents within the completions are rarely paired with safe responses, the model's safeguard against these contents remains underdeveloped. In theory, this bias exists in Supervised Fine-Tuning (SFT), Proximal Policy Optimization (PPO), Direct Preference Optimization (DPO), and more fine-tuning algorithms alike. <br><br>3. Efficacy:  Leveraging this vulnerability, we devise a novel jailbreak method, namely Disguise and Reconstruction Attack (DRA), which achieves over 90% attack success rates on GPT4-web. <br><br> ![image](https://github.com/user-attachments/assets/90436ca7-d650-4e44-a027-5aef5dd01e31) <br><br> <img width="1493" alt="image" src="https://github.com/user-attachments/assets/7b6c9cb5-be44-4885-8d27-1201ba1a379c"> | 
| 《WordGame: Efficient & Effective LLM Jailbreak via Simultaneous Obfuscation in Query and Response》 | [link](https://arxiv.org/pdf/2405.14023v1) | In this paper, we analyze the common pattern of the current safety alignment and show that it is possible to exploit such patterns for jailbreaking attacks by simultaneous obfuscation in queries and responses. Specifically, we propose WordGame attack, which replaces malicious words with word games to break down the adversarial intent of a query and encourage benign content regarding the games to precede the anticipated harmful content in the response, creating a context that is hardly covered by any corpus used for safety alignment. <br><br> ![image](https://github.com/user-attachments/assets/ca644868-a77b-4048-91c8-acddcd01870c) <br><br> Extensive experiments demonstrate that WordGame attack can break the guardrails of the current leading proprietary and open-source LLMs, including the latest Claude 3, GPT 4, and Llama 3 models more effectively than existing attacks efficiently. Further ablation studies on such simultaneous obfuscation in query and response provide evidence of the merits of the attack strategy beyond an individual attack. <br><br> ![image](https://github.com/user-attachments/assets/6113ce39-0ba0-4ce7-83f3-943066f2c667) <br><br> While the current safety alignment measures have proven effective against prevailing jailbreaking attacks, they are still far from perfect. In fact, existing methods often fail to exploit weaknesses in the preference learning pipeline, leaving room for potential adaptive exploitation. Specifically, since preference learning mainly depends on its preference data, namely the malicious queries and the corresponding preferred/non-preferred responses, to correct the model behaviors, the following two caveats naturally arise: (1) During the training process, the LLM becomes overly sensitive to malicious words that frequently appear in safety-related preference data. This bias is then relied upon to guide response generation; (2) The preference learning pipeline only promotes the preferred response over the non-preferred one. However, if neither response is a highly probable response to a query, this learning process would fail to deter jailbreaking behavior. Building upon these two observations, in this paper, we summarize the following two key features of attacks that are crucial to the success of jailbreak. <br><br> ![image](https://github.com/user-attachments/assets/e1b04eeb-9787-42f8-9061-2be8c9540e14) | 
| 《Efficient LLM Jailbreak via Adaptive Dense-to-sparse Constrained Optimization》| [link](https://arxiv.org/abs/2405.09113) | In this paper, we present a new token-level attack method, Adaptive Dense-to-Sparse Constrained Optimization (ADC), which successfully breaches several open-source large language models (LLMs). Our technique transforms the discrete jailbreak optimization problem into a continuous one and progressively enhances the sparsity of the optimizing vectors. This strategy effectively closes the gap between discrete and continuous space optimization. Our experimental findings show that our method outperforms existing token-level techniques in both effectiveness and efficiency. On Harmbench, our method achieves the highest attack success rate on seven out of eight LLMs. Results of transfer attacks on closed-source LLMs will be updated. <br><br> <img width="920" alt="image" src="https://github.com/user-attachments/assets/ad41c56c-7d07-4343-8412-18b7bcfdf487"> | 
| 《Prompt Packer: Deceiving LLMs through Compositional Instruction with Hidden Attacks》| [link](https://arxiv.org/pdf/2310.10077) | In this paper, we introduce an innovative technique for obfuscating harmful instructions: Compositional Instruction Attacks (CIA), which refers to attacking by combination and encapsulation of multiple instructions. <br><br> ![image](https://github.com/user-attachments/assets/38bf6c42-2db3-49d5-a807-fb90aa0b0447) <br><br> ![image](https://github.com/user-attachments/assets/ba90cec1-30e5-4dd1-8be8-fc27a5fd3d99) <br><br> CIA hides harmful prompts within instructions of harmless intentions, making it impossible for the model to identify underlying malicious intentions. Furthermore, we implement two transformation methods, known as T-CIA and W-CIA, to automatically disguise harmful instructions as talking or writing tasks, making them appear harmless to LLMs. <br><br> ![image](https://github.com/user-attachments/assets/04bdda75-95cd-4ce6-bd24-50f650bcacdb) <br><br> ![image](https://github.com/user-attachments/assets/0c234464-93cd-47cb-956b-c289d62edfe3)<br><br> ![image](https://github.com/user-attachments/assets/a1fd36e6-f449-436e-8970-3279c6b1b0fb) <br><br> ![image](https://github.com/user-attachments/assets/f0df42ee-1789-4b08-b99b-2020816e32f5)<br><br>![image](https://github.com/user-attachments/assets/67af63d8-7647-456e-8797-8f56850a79af) <br><br> ![image](https://github.com/user-attachments/assets/e3d6b3bf-998b-4ccf-aad5-d3f2aacedb28) <br><br> We evaluated CIA on GPT-4, ChatGPT, and ChatGLM2 with two safety assessment datasets and two harmful prompt datasets. It achieves an attack success rate of 95%+ on safety assessment datasets, and 83%+ for GPT-4, 91%+ for ChatGPT (gpt-3.5-turbo backed) and ChatGLM2-6B on harmful prompt datasets. Our approach reveals the vulnerability of LLMs to such compositional instruction attacks that harbor underlying harmful intentions, contributing significantly to LLM security development. | 
| 《ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs》| [link](https://arxiv.org/abs/2402.11753) | Safety is critical to the usage of large language models (LLMs). Multiple techniques such as data filtering and supervised fine-tuning have been developed to strengthen LLM safety. However, currently known techniques presume that corpora used for safety alignment of LLMs are solely interpreted by semantics. This assumption, however, does not hold in real-world applications, which leads to severe vulnerabilities in LLMs. For example, users of forums often use ASCII art, a form of text-based art, to convey image information. In this paper, we propose a novel ASCII art-based jailbreak attack and introduce a comprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the capabilities of LLMs in recognizing prompts that cannot be solely interpreted by semantics. <br><br>![image](https://github.com/user-attachments/assets/de8f339d-06b2-4168-9648-2490231240a7)<br><br> We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle to recognize prompts provided in the form of ASCII art. Based on this observation, we develop the jailbreak attack ArtPrompt, which leverages the poor performance of LLMs in recognizing ASCII art to bypass safety measures and elicit undesired behaviors from LLMs. ArtPrompt only requires black-box access to the victim LLMs, making it a practical attack. We evaluate ArtPrompt on five SOTA LLMs, and show that ArtPrompt can effectively and efficiently induce undesired behaviors from all five LLMs. <br><br> ![image](https://github.com/user-attachments/assets/fb1df9db-b3f1-49ca-8966-693d9be2f25f) | 
| 《Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack》| [link](https://arxiv.org/abs/2404.01833) | Large Language Models (LLMs) have risen significantly in popularity and are increasingly being adopted across multiple applications. These LLMs are heavily aligned to resist engaging in illegal or unethical topics as a means to avoid contributing to responsible AI harms. However, a recent line of attacks, known as “jailbreaks”, seek to overcome this alignment. Intuitively, jailbreak attacks aim to narrow the gap between what the model can do and what it is willing to do. In this paper, we introduce a novel jailbreak attack called Crescendo. <br><br>![image](https://github.com/user-attachments/assets/764bbd5e-1ffe-4855-9abc-ee3c261c9e68)<br><br>Unlike existing jailbreak methods, Crescendo is a multi-turn jailbreak that interacts with the model in a seemingly benign manner. It begins with a general prompt or question about the task at hand and then gradually escalates the dialogue by referencing the model’s replies, progressively leading to a successful jailbreak. We evaluate Crescendo on various public systems, including ChatGPT, Gemini Pro, Gemini-Ultra, LlaMA-2 70b Chat, and Anthropic Chat. Our results demonstrate the strong efficacy of Crescendo, with it achieving high attack success rates across all evaluated models and tasks. Furthermore, we introduce Crescendomation, a tool that automates the Crescendo attack, and our evaluation showcases its effectiveness against state-of-the-art models. <br><br>![image](https://github.com/user-attachments/assets/8d32ca6c-d47a-4279-b996-d474906f1822) |
| 《Universal and Transferable Adversarial Attacks on Aligned Language Models》| [link](https://arxiv.org/pdf/2307.15043) |  In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. <br><br><img width="699" alt="image" src="https://github.com/user-attachments/assets/57f85b82-57a3-4eef-a72d-4201b868504b"><br><br>Surprisingly, we find that the adversarial prompts generated by our approach are highly transferable, including to black-box, publicly released, production LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix induces objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. Interestingly, the success rate of this attack transfer is much higher against the GPT-based models, potentially owing to the fact that Vicuna itself is trained on outputs from ChatGPT. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. <br><br><img width="698" alt="image" src="https://github.com/user-attachments/assets/fe07a97a-ab26-4f34-b198-846dccd21514"> |
|《AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models》| [link](https://arxiv.org/abs/2310.04451) | The aligned Large Language Models (LLMs) are powerful language understanding and decision-making tools that are created through extensive alignment with human feedback. However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned LLMs. Investigating jailbreak prompts can lead us to delve into the limitations of LLMs and further guide us to secure them. Unfortunately, existing jailbreak techniques suffer from either (1) scalability issues, where attacks heavily rely on manual crafting of prompts, or (2) stealthiness problems, as attacks depend on token-based algorithms to generate prompts that are often semantically meaningless, making them susceptible to detection through basic perplexity testing. In light of these challenges, we intend to answer this question: Can we develop an approach that can automatically generate stealthy jailbreak prompts? In this paper, we introduce AutoDAN, a novel jailbreak attack against aligned LLMs. AutoDAN can automatically generate stealthy jailbreak prompts by the carefully designed hierarchical genetic algorithm. Extensive evaluations demonstrate that AutoDAN not only automates the process while preserving semantic meaningfulness, but also demonstrates superior attack strength in cross-model transferability, and cross-sample universality compared with the baseline. Moreover, we also compare AutoDAN with perplexity-based defense methods and show that AutoDAN can bypass them effectively. <br><br>![image](https://github.com/user-attachments/assets/7bf861f4-29cd-437f-908f-da60897a1aed) |
| 《DeepInception: Hypnotize Large Language Model to Be Jailbreaker》| [link](https://arxiv.org/abs/2311.03191) | Despite remarkable success in various applications, large language models (LLMs) are vulnerable to adversarial jailbreaks that make the safety guardrails void. However, previous studies for jailbreaks usually resort to brute-force optimization or extrapolations of a high computation cost, which might not be practical or effective. In this paper, inspired by the Milgram experiment w.r.t. the authority power for inciting harmfulness, we disclose a lightweight method, termed as DeepInception, which can easily hypnotize LLM to be a jailbreaker. Specifically, DeepInception leverages the personification ability of LLM to construct a virtual, nested scene to behave, which realizes an adaptive way to escape the usage control in a normal scenario. Empirically, DeepInception can achieve competitive jailbreak success rates with previous counterparts and realize a continuous jailbreak in subsequent interactions, which reveals the critical weakness of self-losing on both open-source and closed-source LLMs like Falcon, Vicuna-v1.5, Llama-2, GPT-3.5, and GPT-4. <br><br><img width="1008" alt="image" src="https://github.com/user-attachments/assets/c5d4310c-fe64-4ef4-b6bd-10bacd4bf4b4"> |
| 《Jailbreaking Black Box Large Language Models in Twenty Queries》 | [link](https://arxiv.org/abs/2310.08419) | There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR—which is inspired by social engineering attacks—uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and Gemini. <br><br>![image](https://github.com/user-attachments/assets/7806a4a4-e78e-4a05-91a4-b94283d1ee31) |
| 《How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs》| [link](https://arxiv.org/abs/2401.06373) | Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused attacks developed by security experts. As large language models (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective on jailbreaking LLMs as human-like communicators to explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. First, we propose a persuasion taxonomy derived from decades of social science research. Then we apply the taxonomy to automatically generate interpretable persuasive adversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over 92% on Llama 2-7b Chat, GPT-3.5, and GPT-4 in 10 trials, surpassing recent algorithm-focused attacks. On the defense side, we explore various mechanisms against PAP, find a significant gap in existing defenses, and advocate for more fundamental mitigation for highly interactive LLMs. <br><br>![image](https://github.com/user-attachments/assets/50ed1678-e8bd-4985-bd01-eb6341f073f1) |
| 《Jailbreaking Proprietary Large Language Models using Word Substitution Cipher》| [link](https://arxiv.org/abs/2402.10601) | Large Language Models (LLMs) are aligned to moral and ethical guidelines but remain susceptible to creative prompts called Jailbreak that can bypass the alignment process. However, most jailbreaking prompts contain harmful questions in the natural language (mainly English), which can be detected by the LLM themselves. In this paper, we present jailbreaking prompts encoded using cryptographic techniques. We first present a pilot study on the state-of-the-art LLM, GPT-4, in decoding several safe sentences that have been encrypted using various cryptographic techniques and find that a straightforward word substitution cipher can be decoded most effectively. Motivated by this result, we use this encoding technique for writing jailbreaking prompts. We present a mapping of unsafe words with safe words and ask the unsafe question using these mapped words. Experimental results show an attack success rate (up to 59.42%) of our proposed jailbreaking approach on state-of-the-art proprietary models including ChatGPT, GPT-4, and Gemini-Pro. Additionally, we discuss the over-defensiveness of these models. We believe that our work will encourage further research in making these LLMs more robust while maintaining their decoding capabilities. <br><br>![image](https://github.com/user-attachments/assets/e3b8bff4-2e83-4002-b483-3129a4aa6126) |
 







