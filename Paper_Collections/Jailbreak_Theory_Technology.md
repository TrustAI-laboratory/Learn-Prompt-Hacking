| Paper Name                                                       | Link                                     | Summary |
|------------------------------------------------------------------|------------------------------------------|----------|
| 《Fundamental limitations of alignment in Large Language Models》 | [link](https://arxiv.org/abs/2304.11082) | In this paper, we introduce a probabilistic framework for analyzing alignment and its limitations in LLMs, which we call Behavior Expectation Bounds (BEB), and use it in order to establish fundamental properties of alignment in LLMs. The core idea behind BEB is to represent the LLM distribution as a superposition of ill- and well-behaved components, in order to provide guarantees on the ability to restrain the ill-behaved components, i.e., guarantees that the LLM is aligned. <br><br>We use this framework to assert several important statements regarding LLM alignment: <br><br>* **(theorem 1)Alignment impossibility**: an LLM alignment process which reduces undesired behaviors to a small but nonzero fraction of the probability space is not safe against adversarial prompts <br>* **(theorem 2)Preset aligning prompts can only provide a finite guardrail against adversarial prompts**: including an aligning prefix prompt does not guarantee alignment. <br>* **(theorem 3)LLMs can be misaligned during a conversation**: a user can misalign an LLM during a conversation, with limited prompt length at each turn. <br>* **(theorem 4)LLMs with best-of-n sampling can be misaligned**: selection of most aligned model response out of n generations, does not guarantee alignment.| 
| 《Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks》 | [link](https://arxiv.org/abs/2404.02151) | In this paper, we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize a target logprob (e.g., of the token “Sure”), potentially with multiple restarts. In this way, we achieve nearly 100% attack success rate. <br><br> ![image](https://github.com/user-attachments/assets/43132cf7-0626-4e66-829b-8c2581422f60)<br>The importance of a well-designed prompt in enhancing the performance of LLMs is well-established. In our approach, we develop a prompt template that can incorporate a generic unsafe request. This template is specifically designed to make the model start from a specified string (e.g., “Sure, here is how to make a bomb”) and steer the model away from its default aligned behavior. Its general structure can be summarized as: <set of rules + harmful request + adversarial suffix>. <br><br>Prior works define adaptive attacks as attacks that are specifically designed to target a given defense. We follow this definition and describe the building blocks of our adaptive attacks, which we combine and potentially adapt depending on the target LLMs. |
| 《Visual Analysis of Jailbreak Attacks Against Large Language Models》 | [link](https://arxiv.org/pdf/2404.08793) | We present a novel LLM-assisted analysis framework coupled with a visual analysis system JailbreakLens to help model practitioners analyze the jailbreak attacks against LLMs. The analysis framework provides a jailbreak result assessment method to evaluate jailbreak performance and supports an in-depth analysis of jailbreak prompt characteristics from component and keyword aspects. The visual system allows users to explore the evaluation results, identify important prompt components and keywords, and verify their effectiveness. A case study, two technical evaluations, and expert interviews show the effectiveness of the analysis framework and visual system. Besides, we distill a set of design implications to inspire future research. <br><br> ![image](https://github.com/user-attachments/assets/c2b3df84-50c4-41be-a317-6ab73bbb70aa) |
| 《Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues》| [link](https://arxiv.org/abs/2304.11082) | This paper presents an indirect approach (Puzzler) to jailbreak LLMs by implicitly expressing malicious intent. Puzzler first combines the wisdom of “When unable to attack, defend” by querying the defensive measures of the original query and attacking them to obtain clues related to the original query. Subsequently, it bypasses the LLM’s safety alignment mechanisms by implicitly expressing the malicious intent of the original query through the combination of diverse clues. <br><br> ![image](https://github.com/user-attachments/assets/03752073-9f6b-476f-8f40-4121da03e2f3) <br><br> The experimental results indicate that the Query Success Rate of the Puzzler is 14.0%-82.7% higher than baselines on the most prominent LLMs. Moreover, when tested against the two state-of-the-art jailbreak detection approaches, only 21.0% jailbreak prompts generated by Puzzler are detected, which is more effective at evading detection compared to baselines. <br><br> ![image](https://github.com/user-attachments/assets/e7719a9d-f407-4e56-b06c-2d85fe22661a) |
| 《Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks》| [link](https://arxiv.org/pdf/2302.05733) | In this work, we show that programmatic capabilities in LLMs allow for convincing generations of malicious content (scams, spam, hate speech, etc.) without any additional training or extensive prompt engineering. Furthermore, we show that simple attacks inspired by computer security can bypass state-of-the-art content filtering deployed in the wild. Our results show the potential for even non-experts to use these systems for malicious purposes, for as little as $0.0064 per generation. We hope that our work spurs further work on viewing LLMs through the lens of traditional computer security, both for attacks and defenses. For example, we hope that future research formalizes security models and provides unconditional defenses under specific threat models. <br><br> ![image](https://github.com/user-attachments/assets/358dd8ae-1d3b-42fc-b097-5db8e39c3d24) |
| 《Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation》 | [link](https://arxiv.org/abs/2311.03348) | This work explores persona-modulation attacks, a general jailbreaking method for state-of-the-art aligned LLMs such as GPT-4 and Claude. Persona-modulation attacks steer the model into adopting a specific personality that is likely to comply with harmful instructions. For example, to circumvent safety measures that prevent misinformation, we steer the model into behaving like an “Aggressive propagandist”. Unlike recent work on adversarial jailbreaks (Zou et al., 2023; Carlini et al., 2023) that are limited to a single prompt-answer pair, persona modulation enables the attacker to enter an unrestricted chat mode that can be used to collaborate with the model on complex tasks that require several steps such as synthesising drugs, building bombs, or laundering money. <br><br> ![image](https://github.com/user-attachments/assets/64f4188a-3560-442c-8322-3e469b972c31) <br><br> Manual persona modulation requires significant effort to produce effective prompts. Therefore, we present automated persona-modulation attacks, a technique that uses an LLM assistant to speed up the creation of jailbreaking prompts. In this setup, the manual effort is reduced to designing a single jailbreak prompt to get GPT-4 to behave as a research assistant. GPT-4 can then create specialised persona-modulation prompts for arbitrary tasks and personas. <br><br> Although automated persona-modulation attacks are fast, they can be less successful at producing harmful completions than manual persona-modulation attacks. To combine the advantages of both approaches, we introduce semi-automated persona modulation attacks. This approach introduces a human-in-the-loop who can modify the outputs of each stage of the automated workflow to maximise the harmfulness of the LLM output. This semi-automated approach recovers the performance of a manual persona-modulation attack, with up to a 25x reduction in time. Overall, we make two contributions. |
| 《Attack Prompt Generation for Red Teaming and Defending Large Language Models》| [link](https://arxiv.org/abs/2310.12505) | In this work, we proposed two frameworks to attack and defend LLMs. <br><br> ![image](https://github.com/user-attachments/assets/5cebc11d-3a17-40ae-b6d9-bc7bfcb0a11b) <br><br> The attack framework combines manual and automatic prompt construction, enabling the generation of more harmful attack prompts compared to previous studies Kang et al. (2023); Zhang et al. (2022). <br><br> ![image](https://github.com/user-attachments/assets/712f9250-3572-48cc-8cc1-63b47c861d49) <br><br> The defense framework fine-tunes the target LLMs by multi-turn interactions with the attack framework. Empirical experiments demonstrate the efficiency and robustness of the defense framework while posing minimal impact on the original capabilities of LLMs. Additionally, we constructed five SAP datasets of attack prompts with varying sizes for safety evaluation and enhancement. In the future, we will construct SAP datasets with more attack prompts and evaluate attack performance on bigger datasets. Besides, we will evaluate more LLMs. |
| 《GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts》| [link](https://arxiv.org/abs/2309.10253) | In this study, we introduced GPTFUZZER, an innovative black-box jailbreak fuzzing framework, drawing inspiration from established frameworks of AFL. Moving beyond the constraints of manual engineering, GPTFUZZER autonomously crafts jailbreak templates, offering a dynamic approach to red teaming LLMs. <br><br> <img width="993" alt="image" src="https://github.com/user-attachments/assets/7c27583e-e2e0-431f-a2e2-1e482ad69acd"> <br>br> Our empirical results underscore the potency of GPTFUZZER in generating these templates, even when initiated with human-written templates of varying quality. This capability not only highlights the robustness of our framework but also underscores potential vulnerabilities in current LLMs. We envision GPTFUZZER serving as a valuable tool for both researchers and industry professionals, facilitating rigorous evaluations of LLM robustness. <br><br> <img width="1012" alt="image" src="https://github.com/user-attachments/assets/006242cb-3b34-4cc0-baad-136e4f7216a0"> |
| 《Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models》| [link](https://arxiv.org/abs/2307.08487) | In conclusion, our research addresses the existing gap in systematic analysis and comprehensive understanding of text safety and output robustness within Large Language Models (LLMs). Through a methodical approach, we have evaluated the safety and robustness of LLMs using a latent jailbreak prompt dataset, incorporating malicious instruction embeddings. By employing a hierarchical annotation framework, we have gained insights into LLM behavior concerning the positioning of explicit normal instructions, word replacements, and instruction replacements. <br><br> ![image](https://github.com/user-attachments/assets/5342dc40-f712-4e0c-983a-d74b392a2b43) <br><br> Our findings underscore that present-day LLMs not only display a propensity for particular instruction verbs but also exhibit varying rates of susceptibility to jailbreaking based on the specific instruction verbs in explicit normal instructions. This implies that the likelihood of generating unsafe content is influenced to differing extents by the instruction verb employed. In essence, the current iteration of LLMs encounters challenges in maintaining both safety and robustness when confronted with latent jailbreak prompts encompassing sensitive subjects. This research not only contributes to a deeper understanding of LLM limitations but also highlights the need for further advancements in enhancing their safety and robustness, particularly when exposed to intricate latent manipulations. <br><br> ![image](https://github.com/user-attachments/assets/e187c2cd-3497-4b7a-b74f-fc709cf4d486) |
| 《Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space》| [link](https://arxiv.org/abs/2402.09063) | Current research in adversarial robustness of LLMs focuses on discrete input manipulations in the natural language space, which can be directly transferred to closed-source models. However, this approach neglects the steady progression of open-source models. As open-source models advance in capability, ensuring their safety also becomes increasingly imperative. Yet, attacks tailored to open-source LLMs that exploit full model access remain largely unexplored. We address this research gap and propose embedding space attack, which directly attacks the continuous embedding representation of input tokens. <br><br> ![image](https://github.com/user-attachments/assets/3c2312fd-bedf-4788-95a7-4ef6f2df6deb) <br><br> We find that embedding space attacks circumvent model alignments and trigger harmful behaviors more efficiently than discrete attacks or model fine-tuning. Furthermore, we present a novel threat model in the context of unlearning and show that embedding space attacks can extract supposedly deleted information from unlearned LLMs across multiple datasets and models. Our findings highlight embedding space attacks as an important threat model in open-source LLMs. ![image](https://github.com/user-attachments/assets/b8d6318c-8d6f-40fd-8c1b-a2a1444233ac) |
| 《GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation》| [link](https://arxiv.org/abs/2405.13077) | Research on jailbreaking has been valuable for testing and understanding the safety and security issues of large language models (LLMs). In this paper, we introduce Iterative Refinement Induced Self-Jailbreak (IRIS), a novel approach that leverages the reflective capabilities of LLMs for jailbreaking with only black-box access. Unlike previous methods, IRIS simplifies the jailbreaking process by using a single model as both the attacker and target. <br><br> ![image](https://github.com/user-attachments/assets/5632a100-a766-4226-a43e-0d392f5b84a8) <br><br> This method first iteratively refines adversarial prompts through self-explanation, which is crucial for ensuring that even well-aligned LLMs obey adversarial instructions. IRIS then rates and enhances the output given the refined prompt to increase its harmfulness. We find IRIS achieves jailbreak success rates of 98% on GPT-4 and 92% on GPT-4 Turbo in under 7 queries. It significantly outperforms prior approaches in automatic, black-box and interpretable jailbreaking, while requiring substantially fewer queries, thereby establishing a new standard for interpretable jailbreaking methods. <br><br> ![image](https://github.com/user-attachments/assets/7a26c974-7954-4902-927d-d21b1e52923e) <br><br> |
| 《Learning Diverse Attacks on Large Language Models for Robust Red-Teaming and Safety Tuning》 | [link](https://arxiv.org/abs/2405.18540) | As LMs become increasingly more capable and widely used, red-teaming them for a wide variety of potential attacks becomes more critical for safe and responsible deployment. We have proposed an approach to generate diverse and effective red-teaming prompts using a novel two-stage procedure consisting of GFlowNet fine-tuning followed by MLE smoothing. Through our experiments, we showed that our approach is effective for red-teaming a wide variety of target LMs with varying levels of safety-tuning. An interesting observation is the transferability of the generated prompts to different target LLMs, which reveals shared failure modes of current approaches for aligning LMs and opens interesting direction for future work. In particular, our reranking-based adaptation procedure can serve as a quick way to red-team new target LLMs during development. <br><br> ![image](https://github.com/user-attachments/assets/5bd97837-bba0-4c48-8061-f40a12e892e2) <br><br> Our approach is not limited to text tokens and future work can explore the applicability to red-team multimodal models (e.g., text-to-image models). Further, an interesting area of future work is extending the approach to the jailbreaking setting, where an attacker language model generates a suffix for an adversarial query prompt. Finally, in addition to red-teaming, it would be interesting to apply our method to generate prompts which can improve model performance on different tasks. |
| 《Efficient LLM-Jailbreaking by Introducing Visual Modality》 | [link](https://arxiv.org/abs/2405.20015) | This paper focuses on jailbreaking attacks against large language models (LLMs), eliciting them to generate objectionable content in response to harmful user queries. Unlike previous LLM-jailbreaks that directly orient to LLMs, our approach begins by constructing a multimodal large language model (MLLM) through the incorporation of a visual module into the target LLM. Subsequently, we conduct an efficient MLLM-jailbreak to generate jailbreaking embeddings embJS. Finally, we convert the embJS into text space to facilitate the jailbreaking of the target LLM. <br><br> ![image](https://github.com/user-attachments/assets/2c961480-2246-4058-88df-e8fdc8440555) <br><br> Compared to direct LLM-jailbreaking, our approach is more efficient, as MLLMs are more vulnerable to jailbreaking than pure LLM. Additionally, to improve the attack success rate (ASR) of jailbreaking, we propose an image-text semantic matching scheme to identify a suitable initial input. Extensive experiments demonstrate that our approach surpasses current state-of-the-art methods in terms of both efficiency and effectiveness. Moreover, our approach exhibits superior cross-class jailbreaking capabilities. <br><br> ![image](https://github.com/user-attachments/assets/62ee799f-9b3b-4adf-b59b-d0b0ff8ebae8) <br><br> |
| 《Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment》 | [link](https://arxiv.org/abs/2308.09662) | This paper focused on safety evaluation and alignment of language models at scale. For evaluation, we proposed a new red-teaming method Red-Eval using a Chain of Utterances (CoU) prompt that could effectively jailbreak not only open-source models such as Vicuna and StableBeluga but also widely used closed-source systems such as GPT-4 and ChatGPT. <br><br>![image](https://github.com/user-attachments/assets/a333e58a-9fa3-4551-9200-f99d57448625) <br><br> With the help of different types of CoU prompting, in Red-Instruct, first, we extracted a conversational dataset, HarmfulQA with harmful questions and safe responses (blue data), and corresponding harmful responses (red data). We used the dataset to perform various safety-alignments of Vicuna-7B to give rise to a new LLM named Starling. An extensive set of experiments shows that Red-Eval outperformed existing red-teaming techniques and jailbreak GPT-4 and ChatGPT for 65% and 73% of the red-teaming attempts. We also show Starling shows safer behavior on safety evaluations while maintaining most of its utility. <br><br>![image](https://github.com/user-attachments/assets/42d89928-58db-4e10-9f42-ef92111f2a3b)|
| 《Don’t Say No: Jailbreaking LLM by Suppressing Refusal》 | [link](https://arxiv.org/abs/2404.16369) | Ensuring the safety alignment of Large Language Models (LLMs) is crucial to generating responses consistent with human values. Despite their ability to recognize and avoid harmful queries, LLMs are vulnerable to "jailbreaking" attacks, where carefully crafted prompts elicit them to produce toxic content. One category of jailbreak attacks is reformulating the task as adversarial attacks by eliciting the LLM to generate an affirmative response. However, the typical attack in this category GCG has very limited attack success rate. In this study, to better study the jailbreak attack, we introduce the DSN (Don’t Say No) attack, which prompts LLMs to not only generate affirmative responses but also novelly enhance the objective to suppress refusals. In addition, another challenge lies in jailbreak attacks is the evaluation, as it is difficult to directly and accurately assess the harmfulness of the attack. The existing evaluation such as refusal keyword matching has its own limitation as it reveals numerous false positive and false negative instances. To overcome this challenge, we propose an ensemble evaluation pipeline incorporating Natural Language Inference (NLI) contradiction assessment and two external LLM evaluators. Extensive experiments demonstrate the potency of the DSN and the effectiveness of ensemble evaluation compared to baseline methods.|
| 《AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs》 | [link](https://arxiv.org/abs/2404.16873) | While recently Large Language Models (LLMs) have achieved remarkable successes, they are vulnerable to certain jailbreaking attacks that lead to generation of inappropriate or harmful content. Manual red-teaming requires finding adversarial prompts that cause such jailbreaking, e.g. by appending a suffix to a given instruction, which is inefficient and time-consuming. On the other hand, automatic adversarial prompt generation often leads to semantically meaningless attacks that can easily be detected by perplexity-based filters, may require gradient information from the TargetLLM, or do not scale well due to time-consuming discrete optimization processes over the token space. In this paper, we present a novel method that uses another LLM, called the AdvPrompter, to generate human-readable adversarial prompts in seconds, ∼800× faster than existing optimization-based approaches. We train the AdvPrompter using a novel algorithm that does not require access to the gradients of the TargetLLM. This process alternates between two steps: (1) generating high-quality target adversarial suffixes by optimizing the AdvPrompter predictions, and (2) low-rank fine-tuning of the AdvPrompter with the generated adversarial suffixes. The trained AdvPrompter generates suffixes that veil the input instruction without changing its meaning, such that the TargetLLM is lured to give a harmful response. Experimental results on popular open source TargetLLMs show state-of-the-art results on the AdvBench dataset, that also transfer to closed-source black-box LLM APIs. Further, we demonstrate that by fine-tuning on a synthetic dataset generated by AdvPrompter, LLMs can be made more robust against jailbreaking attacks while maintaining performance, i.e. high MMLU scores.|
| 《AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models》 | [link](https://arxiv.org/abs/2310.15140) | Safety alignment of Large Language Models (LLMs) can be compromised with manual jailbreak attacks and (automatic) adversarial attacks. Recent studies suggest that defending against these attacks is possible: adversarial attacks generate unlimited but unreadable gibberish prompts, detectable by perplexity-based filters; manual jailbreak attacks craft readable prompts, but their limited number due to the necessity of human creativity allows for easy blocking. In this paper, we show that these solutions may be too optimistic. We introduce AutoDAN, an interpretable, gradient-based adversarial attack that merges the strengths of both attack types. Guided by the dual goals of jailbreak and readability, AutoDAN optimizes and generates tokens one by one from left to right, resulting in readable prompts that bypass perplexity filters while maintaining high attack success rates. Notably, these prompts, generated from scratch using gradients, are interpretable and diverse, with emerging strategies commonly seen in manual jailbreak attacks. They also generalize to unforeseen harmful behaviors and transfer to black-box LLMs better than their unreadable counterparts when using limited training data or a single proxy model. Furthermore, we show the versatility of AutoDAN by automatically leaking system prompts using a customized objective. Our work offers a new way to red-team LLMs and understand jailbreak mechanisms via interpretability. <br><br> ![image](https://github.com/user-attachments/assets/36169d9c-6de4-4598-a508-10b73c948869) | 
| 《DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers》| [link](https://arxiv.org/abs/2402.16914) | The safety alignment of Large Language Models (LLMs) is vulnerable to both manual and automated jailbreak attacks, which adversarially trigger LLMs to output harmful content. However, current methods for jailbreaking LLMs, which nest entire harmful prompts, are not effective at concealing malicious intent and can be easily identified and rejected by well-aligned LLMs. This paper discovers that decomposing a malicious prompt into separated sub-prompts can effectively obscure its underlying malicious intent by presenting it in a fragmented, less detectable form, thereby addressing these limitations. We introduce an automatic prompt Decomposition and Reconstruction framework for jailbreak Attack (DrAttack). DrAttack includes three key components: (a) ‘Decomposition’ of the original prompt into sub-prompts, (b) ‘Reconstruction’ of these sub-prompts implicitly by in-context learning with semantically similar but harmless reassembling demo, and (c) a ‘Synonym Search’ of sub-prompts, aiming to find sub-prompts’ synonyms that maintain the original intent while jailbreaking LLMs. An extensive empirical study across multiple open-source and closed-source LLMs demonstrates that, with a significantly reduced number of queries, DrAttack obtains a substantial gain of success rate over prior SOTA prompt-only attackers. Notably, the success rate of 78.0% on GPT-4 with merely 15 queries surpassed previous art by 33.1%. <br><br> ![image](https://github.com/user-attachments/assets/2fc81c9d-0eb9-49ef-bd35-0f579e178837) |
| 《Tastle: Distract Large Language Models for Automatic Jailbreak Attack》| [link](https://arxiv.org/abs/2402.16914) | In this work, we propose Tastle, a novel jailbreak attack framework that is designed to generate fluent and coherent jailbreak template universal to all malicious queries. Our framework is inspired by the attention mechanisms of LLMs and consists of three components: concealing malicious content, memory reframing, and optimization algorithms. We investigate the effectiveness of Tastle on five language models, both open-source and closed-source. <br><br> ![image](https://github.com/user-attachments/assets/425b049b-1db9-4b09-bea7-f4dd7dec6b69) <br><br> Experiment results show strong attack success rates of Tastle for direct attacks and transferred attacks across target models and malicious queries. As LLMs become more capable and widely used, it becomes increasingly crucial to have informed assessments of model safety, including disclosing the covert cases in which the LLMs may fail. We thus view our work on automatic jailbreak attack as a step towards this goal. <br><br> ![image](https://github.com/user-attachments/assets/05060cfa-2237-43ed-8d0b-46cc99d91dbe) |
| 《Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak》| [link](https://arxiv.org/abs/2312.04127) | Extensive work has been devoted to improving the safety mechanism of Large Language Models (LLMs). However, LLMs still tend to generate harmful responses when faced with malicious instructions, a phenomenon referred to as “Jailbreak Attack”. In our research, we introduce a novel automatic jailbreak method RADIAL, which bypasses the security mechanism by amplifying the potential of LLMs to generate affirmation responses. The jailbreak idea of our method is “Inherent Response Tendency Analysis” which identifies real-world instructions that can inherently induce LLMs to generate affirmation responses and the corresponding jailbreak strategy is “Real-World Instructions-Driven Jailbreak” which involves strategically splicing real-world instructions identified through the above analysis around the malicious instruction. Our method achieves excellent attack performance on English malicious instructions with five open-source advanced LLMs while maintaining robust attack performance in executing cross-language attacks against Chinese malicious instructions. We conduct experiments to verify the effectiveness of our jailbreak idea and the rationality of our jailbreak strategy design. Notably, our method designed a semantically coherent attack prompt, highlighting the potential risks of LLMs. Our study provides detailed insights into jailbreak attacks, establishing a foundation for the development of safer LLMs. <br><br> ![image](https://github.com/user-attachments/assets/d7f3983c-56bd-4f6a-bb1e-d1a13726585b) |
| 《WHEN LLM MEETS DRL: ADVANCING JAILBREAKING EFFICIENCY VIA DRL-GUIDED SEARCH》 | [link](https://arxiv.org/pdf/2406.08705) | We introduce RLbreaker, a DRL-driven black-box jailbreaking attack. We model LLM jailbreaking as a searching problem and design a DRL agent to guide efficient search. Our DRL agent enables deterministic search, which reduces the randomness and improves the search efficiency compared to existing stochastic search-based attacks Technically speaking, we design specific reward function, actions, and states for our agents, as well as a customized learning algorithm. We empirically demonstrate that RLbreaker outperforms existing attacks in jailbreaking different LLMs, including the very large model, Llama-2-70B. We also validate RLbreaker’s resiliency against SOTA defenses and its ability to transfer across different models. A thorough ablation study underscores the importance of RLbreaker’s core designs, revealing its robustness to changes in critical hyperparameters. These findings verify DRL’s efficacy for automatically generating jailbreaking prompts against LLMs. <br><br> <img width="938" alt="image" src="https://github.com/user-attachments/assets/8ea13678-c5e6-404d-a648-a9092a1b7eed"> |
| 《Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction》| [paper](https://arxiv.org/abs/2402.18104); [blog](https://sites.google.com/view/dra-jailbreak/) |  Question: Why is LLM easily jailbroken after it utters words like "Sure, here is the plan for how to rob a bank"? Researches like GCG have observed this phenomenon, yet few of them have explored its underlying mechanism. <br><br> We recognize it as a vulnerability and localize its root cause in the fine-tuning bias: <br><br> 1. The vulnerability: Given a piece of harmful content (i.e. an inducing context followed by a harmful instruction), the LLM is more likely to endorse it when it appears within the completion rather than query. <br><br> 2. The bias: In the fine-tuning data, harmful content within the queries is likely to co-occur with safe responses, which is not the case for harmful content within completions. Since harmful contents within the completions are rarely paired with safe responses, the model's safeguard against these contents remains underdeveloped. In theory, this bias exists in Supervised Fine-Tuning (SFT), Proximal Policy Optimization (PPO), Direct Preference Optimization (DPO), and more fine-tuning algorithms alike. <br><br>3. Efficacy:  Leveraging this vulnerability, we devise a novel jailbreak method, namely Disguise and Reconstruction Attack (DRA), which achieves over 90% attack success rates on GPT4-web. <br><br> ![image](https://github.com/user-attachments/assets/90436ca7-d650-4e44-a027-5aef5dd01e31) <br><br> <img width="1493" alt="image" src="https://github.com/user-attachments/assets/7b6c9cb5-be44-4885-8d27-1201ba1a379c"> | 
| 《WordGame: Efficient & Effective LLM Jailbreak via Simultaneous Obfuscation in Query and Response》 | [link](https://arxiv.org/pdf/2405.14023v1) | In this paper, we analyze the common pattern of the current safety alignment and show that it is possible to exploit such patterns for jailbreaking attacks by simultaneous obfuscation in queries and responses. Specifically, we propose WordGame attack, which replaces malicious words with word games to break down the adversarial intent of a query and encourage benign content regarding the games to precede the anticipated harmful content in the response, creating a context that is hardly covered by any corpus used for safety alignment. <br><br> ![image](https://github.com/user-attachments/assets/ca644868-a77b-4048-91c8-acddcd01870c) <br><br> Extensive experiments demonstrate that WordGame attack can break the guardrails of the current leading proprietary and open-source LLMs, including the latest Claude 3, GPT 4, and Llama 3 models more effectively than existing attacks efficiently. Further ablation studies on such simultaneous obfuscation in query and response provide evidence of the merits of the attack strategy beyond an individual attack. <br><br> ![image](https://github.com/user-attachments/assets/6113ce39-0ba0-4ce7-83f3-943066f2c667) <br><br> While the current safety alignment measures have proven effective against prevailing jailbreaking attacks, they are still far from perfect. In fact, existing methods often fail to exploit weaknesses in the preference learning pipeline, leaving room for potential adaptive exploitation. Specifically, since preference learning mainly depends on its preference data, namely the malicious queries and the corresponding preferred/non-preferred responses, to correct the model behaviors, the following two caveats naturally arise: (1) During the training process, the LLM becomes overly sensitive to malicious words that frequently appear in safety-related preference data. This bias is then relied upon to guide response generation; (2) The preference learning pipeline only promotes the preferred response over the non-preferred one. However, if neither response is a highly probable response to a query, this learning process would fail to deter jailbreaking behavior. Building upon these two observations, in this paper, we summarize the following two key features of attacks that are crucial to the success of jailbreak. <br><br> ![image](https://github.com/user-attachments/assets/e1b04eeb-9787-42f8-9061-2be8c9540e14) | 
| 《Efficient LLM Jailbreak via Adaptive Dense-to-sparse Constrained Optimization》| [link](https://arxiv.org/abs/2405.09113) | In this paper, we present a new token-level attack method, Adaptive Dense-to-Sparse Constrained Optimization (ADC), which successfully breaches several open-source large language models (LLMs). Our technique transforms the discrete jailbreak optimization problem into a continuous one and progressively enhances the sparsity of the optimizing vectors. This strategy effectively closes the gap between discrete and continuous space optimization. Our experimental findings show that our method outperforms existing token-level techniques in both effectiveness and efficiency. On Harmbench, our method achieves the highest attack success rate on seven out of eight LLMs. Results of transfer attacks on closed-source LLMs will be updated. <br><br> <img width="920" alt="image" src="https://github.com/user-attachments/assets/ad41c56c-7d07-4343-8412-18b7bcfdf487"> | 
| 《Prompt Packer: Deceiving LLMs through Compositional Instruction with Hidden Attacks》| [link](https://arxiv.org/pdf/2310.10077) | In this paper, we introduce an innovative technique for obfuscating harmful instructions: Compositional Instruction Attacks (CIA), which refers to attacking by combination and encapsulation of multiple instructions. <br><br> ![image](https://github.com/user-attachments/assets/38bf6c42-2db3-49d5-a807-fb90aa0b0447) <br><br> ![image](https://github.com/user-attachments/assets/ba90cec1-30e5-4dd1-8be8-fc27a5fd3d99) <br><br> CIA hides harmful prompts within instructions of harmless intentions, making it impossible for the model to identify underlying malicious intentions. Furthermore, we implement two transformation methods, known as T-CIA and W-CIA, to automatically disguise harmful instructions as talking or writing tasks, making them appear harmless to LLMs. <br><br> ![image](https://github.com/user-attachments/assets/04bdda75-95cd-4ce6-bd24-50f650bcacdb) <br><br> ![image](https://github.com/user-attachments/assets/0c234464-93cd-47cb-956b-c289d62edfe3)<br><br> ![image](https://github.com/user-attachments/assets/a1fd36e6-f449-436e-8970-3279c6b1b0fb) <br><br> ![image](https://github.com/user-attachments/assets/f0df42ee-1789-4b08-b99b-2020816e32f5)<br><br>![image](https://github.com/user-attachments/assets/67af63d8-7647-456e-8797-8f56850a79af) <br><br> ![image](https://github.com/user-attachments/assets/e3d6b3bf-998b-4ccf-aad5-d3f2aacedb28) <br><br> We evaluated CIA on GPT-4, ChatGPT, and ChatGLM2 with two safety assessment datasets and two harmful prompt datasets. It achieves an attack success rate of 95%+ on safety assessment datasets, and 83%+ for GPT-4, 91%+ for ChatGPT (gpt-3.5-turbo backed) and ChatGLM2-6B on harmful prompt datasets. Our approach reveals the vulnerability of LLMs to such compositional instruction attacks that harbor underlying harmful intentions, contributing significantly to LLM security development. | 
| 《ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs》| [link](https://arxiv.org/abs/2402.11753) | Safety is critical to the usage of large language models (LLMs). Multiple techniques such as data filtering and supervised fine-tuning have been developed to strengthen LLM safety. However, currently known techniques presume that corpora used for safety alignment of LLMs are solely interpreted by semantics. This assumption, however, does not hold in real-world applications, which leads to severe vulnerabilities in LLMs. For example, users of forums often use ASCII art, a form of text-based art, to convey image information. In this paper, we propose a novel ASCII art-based jailbreak attack and introduce a comprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the capabilities of LLMs in recognizing prompts that cannot be solely interpreted by semantics. <br><br>![image](https://github.com/user-attachments/assets/de8f339d-06b2-4168-9648-2490231240a7)<br><br> We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle to recognize prompts provided in the form of ASCII art. Based on this observation, we develop the jailbreak attack ArtPrompt, which leverages the poor performance of LLMs in recognizing ASCII art to bypass safety measures and elicit undesired behaviors from LLMs. ArtPrompt only requires black-box access to the victim LLMs, making it a practical attack. We evaluate ArtPrompt on five SOTA LLMs, and show that ArtPrompt can effectively and efficiently induce undesired behaviors from all five LLMs. <br><br> ![image](https://github.com/user-attachments/assets/fb1df9db-b3f1-49ca-8966-693d9be2f25f) | 
| 《Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack》| [link](https://arxiv.org/abs/2404.01833) | Large Language Models (LLMs) have risen significantly in popularity and are increasingly being adopted across multiple applications. These LLMs are heavily aligned to resist engaging in illegal or unethical topics as a means to avoid contributing to responsible AI harms. However, a recent line of attacks, known as “jailbreaks”, seek to overcome this alignment. Intuitively, jailbreak attacks aim to narrow the gap between what the model can do and what it is willing to do. In this paper, we introduce a novel jailbreak attack called Crescendo. <br><br>![image](https://github.com/user-attachments/assets/764bbd5e-1ffe-4855-9abc-ee3c261c9e68)<br><br>Unlike existing jailbreak methods, Crescendo is a multi-turn jailbreak that interacts with the model in a seemingly benign manner. It begins with a general prompt or question about the task at hand and then gradually escalates the dialogue by referencing the model’s replies, progressively leading to a successful jailbreak. We evaluate Crescendo on various public systems, including ChatGPT, Gemini Pro, Gemini-Ultra, LlaMA-2 70b Chat, and Anthropic Chat. Our results demonstrate the strong efficacy of Crescendo, with it achieving high attack success rates across all evaluated models and tasks. Furthermore, we introduce Crescendomation, a tool that automates the Crescendo attack, and our evaluation showcases its effectiveness against state-of-the-art models. <br><br>![image](https://github.com/user-attachments/assets/8d32ca6c-d47a-4279-b996-d474906f1822) |
| 《Universal and Transferable Adversarial Attacks on Aligned Language Models》| [link](https://arxiv.org/pdf/2307.15043) |  In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. <br><br><img width="699" alt="image" src="https://github.com/user-attachments/assets/57f85b82-57a3-4eef-a72d-4201b868504b"><br><br>Surprisingly, we find that the adversarial prompts generated by our approach are highly transferable, including to black-box, publicly released, production LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix induces objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. Interestingly, the success rate of this attack transfer is much higher against the GPT-based models, potentially owing to the fact that Vicuna itself is trained on outputs from ChatGPT. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. <br><br><img width="698" alt="image" src="https://github.com/user-attachments/assets/fe07a97a-ab26-4f34-b198-846dccd21514"> |
|《AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models》| [link](https://arxiv.org/abs/2310.04451) | The aligned Large Language Models (LLMs) are powerful language understanding and decision-making tools that are created through extensive alignment with human feedback. However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned LLMs. Investigating jailbreak prompts can lead us to delve into the limitations of LLMs and further guide us to secure them. Unfortunately, existing jailbreak techniques suffer from either (1) scalability issues, where attacks heavily rely on manual crafting of prompts, or (2) stealthiness problems, as attacks depend on token-based algorithms to generate prompts that are often semantically meaningless, making them susceptible to detection through basic perplexity testing. In light of these challenges, we intend to answer this question: Can we develop an approach that can automatically generate stealthy jailbreak prompts? In this paper, we introduce AutoDAN, a novel jailbreak attack against aligned LLMs. AutoDAN can automatically generate stealthy jailbreak prompts by the carefully designed hierarchical genetic algorithm. Extensive evaluations demonstrate that AutoDAN not only automates the process while preserving semantic meaningfulness, but also demonstrates superior attack strength in cross-model transferability, and cross-sample universality compared with the baseline. Moreover, we also compare AutoDAN with perplexity-based defense methods and show that AutoDAN can bypass them effectively. <br><br>![image](https://github.com/user-attachments/assets/7bf861f4-29cd-437f-908f-da60897a1aed) |
| 《DeepInception: Hypnotize Large Language Model to Be Jailbreaker》| [link](https://arxiv.org/abs/2311.03191) | Despite remarkable success in various applications, large language models (LLMs) are vulnerable to adversarial jailbreaks that make the safety guardrails void. However, previous studies for jailbreaks usually resort to brute-force optimization or extrapolations of a high computation cost, which might not be practical or effective. In this paper, inspired by the Milgram experiment w.r.t. the authority power for inciting harmfulness, we disclose a lightweight method, termed as DeepInception, which can easily hypnotize LLM to be a jailbreaker. Specifically, DeepInception leverages the personification ability of LLM to construct a virtual, nested scene to behave, which realizes an adaptive way to escape the usage control in a normal scenario. Empirically, DeepInception can achieve competitive jailbreak success rates with previous counterparts and realize a continuous jailbreak in subsequent interactions, which reveals the critical weakness of self-losing on both open-source and closed-source LLMs like Falcon, Vicuna-v1.5, Llama-2, GPT-3.5, and GPT-4. <br><br><img width="1008" alt="image" src="https://github.com/user-attachments/assets/c5d4310c-fe64-4ef4-b6bd-10bacd4bf4b4"> |
| 《Jailbreaking Black Box Large Language Models in Twenty Queries》 | [link](https://arxiv.org/abs/2310.08419) | There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR—which is inspired by social engineering attacks—uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and Gemini. <br><br>![image](https://github.com/user-attachments/assets/7806a4a4-e78e-4a05-91a4-b94283d1ee31) |
| 《How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs》| [link](https://arxiv.org/abs/2401.06373) | Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused attacks developed by security experts. As large language models (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective on jailbreaking LLMs as human-like communicators to explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. First, we propose a persuasion taxonomy derived from decades of social science research. Then we apply the taxonomy to automatically generate interpretable persuasive adversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over 92% on Llama 2-7b Chat, GPT-3.5, and GPT-4 in 10 trials, surpassing recent algorithm-focused attacks. On the defense side, we explore various mechanisms against PAP, find a significant gap in existing defenses, and advocate for more fundamental mitigation for highly interactive LLMs. <br><br>![image](https://github.com/user-attachments/assets/50ed1678-e8bd-4985-bd01-eb6341f073f1) |
| 《Jailbreaking Proprietary Large Language Models using Word Substitution Cipher》| [link](https://arxiv.org/abs/2402.10601) | Large Language Models (LLMs) are aligned to moral and ethical guidelines but remain susceptible to creative prompts called Jailbreak that can bypass the alignment process. However, most jailbreaking prompts contain harmful questions in the natural language (mainly English), which can be detected by the LLM themselves. In this paper, we present jailbreaking prompts encoded using cryptographic techniques. We first present a pilot study on the state-of-the-art LLM, GPT-4, in decoding several safe sentences that have been encrypted using various cryptographic techniques and find that a straightforward word substitution cipher can be decoded most effectively. Motivated by this result, we use this encoding technique for writing jailbreaking prompts. We present a mapping of unsafe words with safe words and ask the unsafe question using these mapped words. Experimental results show an attack success rate (up to 59.42%) of our proposed jailbreaking approach on state-of-the-art proprietary models including ChatGPT, GPT-4, and Gemini-Pro. Additionally, we discuss the over-defensiveness of these models. We believe that our work will encourage further research in making these LLMs more robust while maintaining their decoding capabilities. <br><br>![image](https://github.com/user-attachments/assets/e3b8bff4-2e83-4002-b483-3129a4aa6126) |
| 《Images are Achilles’ Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models》| [link](https://arxiv.org/abs/2403.09792) | In this paper, we conducted a comprehensive empirical analysis of the harmlessness alignment of MLLMs, specifically examining the visual vulnerabilities for jailbreak. <br><br>![image](https://github.com/user-attachments/assets/5e8890c4-77e4-4cb1-b6a6-e38566dfe0c5) <br><br> Our findings revealed that images pose significant vulnerabilities in the alignment of MLLMs: the presence of images, the cross-modal finetuning process, and the harmfulness of images all contribute to an increased propensity for MLLMs to generate harmful responses. Furthermore, we introduced HADES, a novel jailbreaking approach that hides and amplifies the harmfulness of textual instructions using meticulously crafted images. Extensive experiments have demonstrated that HADES is capable of effectively jailbreaking both open- and closed-source MLLMs. In summary, our work has presented strong evidence that the visual modality poses the alignment vulnerability of MLLMs, underscoring the urgent need for further exploration into cross-modal alignment. <br><br> ![image](https://github.com/user-attachments/assets/e5de13b3-dc43-4534-a5bd-1d2a32a4b1bb) | 
| 《Muting Whisper: A Universal Acoustic Adversarial Attack on Speech Foundation Models》| [link](https://arxiv.org/abs/2405.06134) | Recent developments in large speech foundation models like Whisper have led to their widespread use in many automatic speech recognition (ASR) applications. These systems incorporate ‘special tokens’ in their vocabulary, such as <endoftext>, to guide their language generation process. However, we demonstrate that these tokens can be exploited by adversarial attacks to manipulate the model’s behavior. We propose a simple yet effective method to learn a universal acoustic realization of Whisper’s <endoftext> token, which, when prepended to any speech signal, encourages the model to ignore the speech and only transcribe the special token, effectively ‘muting’ the model. Our experiments demonstrate that the same, universal 0.64-second adversarial audio segment can successfully mute a target Whisper ASR model for over 97% of speech samples. Moreover, we find that this universal adversarial audio segment often transfers to new datasets and tasks. Overall this work demonstrates the vulnerability of Whisper models to ‘muting’ adversarial attacks, where such attacks can pose both risks and potential benefits in real-world settings: for example the attack can be used to bypass speech moderation systems, or conversely the attack can also be used to protect private speech data. <br><br> ![image](https://github.com/user-attachments/assets/8fd2da45-c6c3-4ece-b80b-ad90d24c319c) |
| 《FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts》| [link](https://arxiv.org/pdf/2311.05608) | In this paper, we introduce FigStep, a straightforward yet effective jailbreaking algorithm against VLMs. <br><br>![image](https://github.com/user-attachments/assets/a4ed8096-d41b-45d1-9cd5-5186b65ab6fa)<br><br> Our approach is centered on transforming harmful textual instructions into typographic images, circumventing the safety alignment in the underlying LLMs of VLMs. <br><br>![image](https://github.com/user-attachments/assets/2a07b339-e609-489b-994d-ce78789b19dc)<br><br>FigStep has led to an impressive average Attack Success Rate (ASR) of up to 82.50% across six popular open-source VLMs. By comparing the results of the vanilla query and FigStep, we uncover cross-modality alignment vulnerabilities of VLMs, which highlights the critical need for more sophisticated alignment approaches to improve the safety and reliability of VLMs. For instance, FigStep could jailbreak MiniGPT4-Llama-2-CHAT7B, whose underlying language module, LLaMA-2-Chat-7B, performs excellent safety restrictions for text-only harmful queries. Furthermore, we propose an upgraded version, FigStep-Pro, to jailbreak GPT-4V, the state-of-the-art VLM. FigStep-Pro leverages strategic segmentation technique to effectively circumvent the OCR detector employed in GPT-4V. Eventually, FigStep-Pro achieves a remarkably high ASR of 70% for jailbreaking GPT-4V. Above all, we highlight that it is dangerous and irresponsible to directly release the VLMs without ensuring strict cross-modal alignment. <br><br>![image](https://github.com/user-attachments/assets/18d841d8-8729-4bc1-8c6c-54ace323d434) | 
| 《Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment》| [link](https://arxiv.org/abs/2308.09662) | Larger language models (LLMs) have taken the world by storm with their massive multi-tasking capabilities simply by optimizing over a next-word prediction objective. With the emergence of their properties and encoded knowledge, the risk of LLMs producing harmful outputs increases, making them unfit for scalable deployment for the public. In this work, we propose a new safety evaluation benchmark Red-Eval that carries out red-teaming. We show that even widely deployed models are susceptible to the Chain of Utterances-based (CoU) prompting, jailbreaking closed source LLM-based systems such as GPT-4 and ChatGPT to unethically respond to more than 65% and 73% of harmful queries. We also demonstrate the consistency of the Red-Eval across 8 open-source LLMs in generating harmful responses in more than 86% of the red-teaming attempts. Next, we propose Red-Instruct—An approach for safety alignment of LLMs. It constitutes two phases: 1) HarmfulQA data collection: Leveraging CoU prompting, we collect a dataset that consists of 1.9K harmful questions covering a wide range of topics, 9.5K safe and 7.3K harmful conversations from ChatGPT; 2) Safe-Align: We demonstrate how the conversational dataset can be used for the safety alignment of LLMs by minimizing the negative log-likelihood over helpful responses and penalizing over harmful responses by gradient ascent over sample loss. Our model Starling, a fine-tuned Vicuna-7B, is observed to be more safety aligned when evaluated on Red-Eval and HHH benchmarks while preserving the utility of the baseline models (TruthfulQA, MMLU, and BBH). <br><br>![image](https://github.com/user-attachments/assets/8bd73c67-2697-419a-8715-cfd82794ae9e) |
| 《GPT-4 IS TOO SMART TO BE SAFE: STEALTHY CHAT WITH LLMS VIA CIPHER》| [link](https://arxiv.org/pdf/2308.06463) | Safety lies at the core of the development of Large Language Models(LLMs). There is ample work on aligning LLMs with human ethics and preferences, including data filtering in pretraining, supervised fine-tuning, reinforcement learning from human feedback, red teaming, etc. In this study, we discover that chat in cipher can bypass the safety alignment techniques of LLMs, which are mainly conducted in natural languages. We propose a novel framework CipherChat to systematically examine the generalizability of safety alignment to non-natural languages – ciphers. <br><br><img width="810" alt="image" src="https://github.com/user-attachments/assets/994ed108-288d-4e88-892b-91f55dbca0cc" /> <br><br> CipherChat enables humans to chat with LLMs through cipher prompts topped with system role descriptions and few-shot enciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs, including ChatGPT and GPT-4 for different representative human ciphers across 11 safety domains in both English and Chinese. Experimental results show that certain ciphers succeed almost 100% of the time in bypassing the safety alignment of GPT-4 in several safety domains, demonstrating the necessity of developing safety alignment for non-natural languages. Notably, we identify that LLMs seem to have a “secret cipher”, and propose a novel SelfCipher that uses only role play and several unsafe demonstrations in natural language to evoke this capability. SelfCipher surprisingly outperforms existing human ciphers in almost all cases. <br><br> <img width="755" alt="image" src="https://github.com/user-attachments/assets/169f8d6e-024c-4a4e-b625-67d8123513af" /> |
| 《MULTILINGUAL JAILBREAK CHALLENGES IN LARGE LANGUAGE MODELS》| [link](https://arxiv.org/pdf/2310.06474) | Although several preventive measures have been developed to mitigate the potential risks associated with LLMs, they have primarily focused on English. In this study, we reveal the presence of multilingual jailbreak challenges within LLMs and consider two potential risky scenarios: unintentional and intentional. The unintentional scenario involves users querying LLMs using non-English prompts and inadvertently bypassing the safety mechanisms, while the intentional scenario concerns malicious users combining malicious instructions with multilingual prompts to deliberately attack LLMs. <br><br><img width="1064" alt="image" src="https://github.com/user-attachments/assets/1eb4f468-904e-4132-bdc2-8f46f99d71da" /> | 
| 《CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models》 | [link](https://arxiv.org/abs/2402.16717) | This paper explores jailbreaking in language models. After analyzing current methods, we propose a hypothesis on LLMs’ safety mechanisms: first detecting intent, then generating responses. Based on this, we introduce CodeChameleon, a framework that encrypts and decrypts queries into a form difficult for LLMs to detect. Our extensive tests reveal CodeChameleon successfully evades LLMs’ intent recognition. Across seven major LLMs, it achieves an average attack success rate (ASR) of 77.5%, a significant 29.8% increase over the best existing method. <br><br> ![image](https://github.com/user-attachments/assets/1929c08f-ca29-4b26-b20a-9fde9be10f47) |
| 《Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations》 | [link](https://arxiv.org/abs/2310.06387) | In this paper, we uncover the power of in-context demonstrations in manipulating the alignment ability of LLMs for both attack and defense purposes by the proposed two techniques: In-Context Attack (ICA) and In-Context Defense (ICD). For ICA, we show that a few demonstrations of responding to malicious prompts can jailbreak the model to generate harmful content. On the other hand, ICD enhances model robustness by demonstrations of rejecting harmful prompts. We also provide theoretical justifications to understand the effectiveness of only a few adversarial demonstrations. Our comprehensive evaluations illustrate the practicality and effectiveness of ICA and ICD, highlighting their significant potential on LLMs alignment and security and providing a new perspective to study this issue. <br><br> ![image](https://github.com/user-attachments/assets/4878bb68-f33a-433e-9f72-c4916b450274) |
| 《Jailbroken: How Does LLM Safety Training Fail?》 | [link](https://arxiv.org/pdf/2307.02483) | Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of “jailbreak” attacks on early releases of ChatGPT that elicit undesired behavior. Going beyond recognition of the issue, we investigate why such attacks succeed and how they can be created. We hypothesize two failure modes of safety training: competing objectives and mismatched generalization. Competing objectives arise when a model’s capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist. We use these failure modes to guide jailbreak design and then evaluate state-of-the-art models, including OpenAI’s GPT-4 and Anthropic’s Claude v1.3, against both existing and newly designed attacks. We find that vulnerabilities persist despite the extensive red-teaming and safety-training efforts behind these models. Notably, new attacks utilizing our failure modes succeed on every prompt in a collection of unsafe requests from the models’ red-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our analysis emphasizes the need for safety-capability parity—that safety mechanisms should be as sophisticated as the underlying model—and argues against the idea that scaling alone can resolve these safety failure modes. <br><br> <img width="1001" alt="image" src="https://github.com/user-attachments/assets/7bffa895-dc7a-4654-914d-5242f0b9dfb4" /> |
| 《Multi-step Jailbreaking Privacy Attacks on ChatGPT》 | [link](https://arxiv.org/abs/2304.05197) | With the rapid progress of large language models (LLMs), many downstream NLP tasks can be well solved given appropriate prompts. Though model developers and researchers work hard on dialog safety to avoid generating harmful content from LLMs, it is still challenging to steer AI-generated content (AIGC) for the human good. As powerful LLMs are devouring existing text data from various domains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether the private information is included in the training data and what privacy threats can these LLMs and their downstream applications bring. In this paper, we study the privacy threats from OpenAI’s ChatGPT and the New Bing enhanced by ChatGPT and show that application-integrated LLMs may cause new privacy threats. To this end, we conduct extensive experiments to support our claims and discuss LLMs’ privacy implications. <br><br> <img width="851" alt="image" src="https://github.com/user-attachments/assets/b679b3df-a0db-4fed-b981-ade10e1d2865" /> | 
| 《A Wolf in Sheep’s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily》| [link](https://arxiv.org/pdf/2311.08268) | Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to provide useful and safe responses. However, adversarial prompts known as ‘jailbreaks’ can circumvent safeguards, leading LLMs to generate potentially harmful content. Exploring jailbreak prompts can help to better reveal the weaknesses of LLMs and further steer us to secure them. Unfortunately, existing jailbreak methods either suffer from intricate manual design or require optimization on other white-box models, which compromises either generalization or efficiency. In this paper, we generalize jailbreak prompt attacks into two aspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM, an automatic framework that leverages LLMs themselves to generate effective jailbreak prompts. Extensive experiments demonstrate that ReNeLLM significantly improves the attack success rate while greatly reducing the time cost compared to existing baselines. Our study also reveals the inadequacy of current defense methods in safeguarding LLMs. Finally, we analyze the failure of LLMs defense from the perspective of prompt execution priority, and propose corresponding defense strategies. We hope that our research can catalyze both the academic community and LLMs developers towards the provision of safer and more regulated LLMs. The code is available at https://github.com/NJUNLP/ReNeLLM. <br><br> ![image](https://github.com/user-attachments/assets/763af1cb-1c38-4d25-8a4f-8ba2e381832a) <br><br> ![image](https://github.com/user-attachments/assets/28abc9f2-0eeb-49e7-bf9d-b8c55b00ba56) | 
| 《Tree of Attacks: Jailbreaking Black-Box LLMs Automatically》| [link](https://arxiv.org/abs/2312.02119) | While Large Language Models (LLMs) display versatile functionality, they continue to generate harmful, biased, and toxic content, as demonstrated by the prevalence of human-designed jailbreaks. In this work, we present Tree of Attacks with Pruning (TAP), an automated method for generating jailbreaks that only requires black-box access to the target LLM. TAP utilizes an attacker LLM to iteratively refine candidate (attack) prompts until one of the refined prompts jailbreaks the target. In addition, before sending prompts to the target, TAP assesses them and prunes the ones unlikely to result in jailbreaks, reducing the number of queries sent to the target LLM. In empirical evaluations, we observe that TAP generates prompts that jailbreak state-of-the-art LLMs (including GPT4-Turbo and GPT4o) for more than 80% of the prompts. This significantly improves upon the previous state-of-the-art black-box methods for generating jailbreaks while using a smaller number of queries than them. Furthermore, TAP is also capable of jailbreaking LLMs protected by state-of-the-art guardrails, e.g., LlamaGuard. <br><br> ![image](https://github.com/user-attachments/assets/4d528274-eba2-41fc-9aa6-a766e52acea3) | 
| 《Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models》| [link](https://arxiv.org/abs/2407.16205) | The rapid development of Large Language Models (LLMs) has brought remarkable generative capabilities across diverse tasks. However, despite the impressive achievements, these LLMs still have numerous inherent vulnerabilities, particularly when faced with jailbreak attacks. By investigating jailbreak attacks, we can uncover hidden weaknesses in LLMs and inform the development of more robust defense mechanisms to fortify their security. In this paper, we further explore the jailbreak attacks on LLMs and propose Analyzing-based Jailbreak (ABJ). This effective jailbreak attack method takes advantage of LLMs’ growing analyzing and reasoning capability and reveals their underlying vulnerabilities when facing analyzing-based tasks. We conduct a detailed evaluation of ABJ across various open-source and closed-source LLMs, which achieves 94.8% attack success rate (ASR) and 1.06 attack efficiency (AE) on GPT-4-turbo-0409, demonstrating state-of-the-art attack effectiveness and efficiency. Our research highlights the importance of prioritizing and enhancing the safety of LLMs to mitigate the risks of misuse. The code is publicly available at https://github.com/theshi-1128/ABJ-Attack. <br><br> ![image](https://github.com/user-attachments/assets/f3ba7b9a-d786-4032-8167-46ebe7991b77) |
| 《MasterKey: Automated Jailbreak Across Multiple Large Language Model Chatbots》 | [link](https://arxiv.org/abs/2307.08715) | arge language models (LLMs), such as chatbots, have made significant strides in various fields but remain vulnerable to jailbreak attacks, which aim to elicit inappropriate responses. Despite efforts to identify these weaknesses, current strategies are ineffective against mainstream LLM chatbots, mainly due to undisclosed defensive measures by service providers. Our paper introduces MASTERKEY, a framework exploring the dynamics of jailbreak attacks and countermeasures. We present a novel method based on time-based characteristics to dissect LLM chatbot defenses. This technique, inspired by time-based SQL injection, uncovers the workings of these defenses and demonstrates a proof-of-concept attack on several LLM chatbots. <br><br> <img width="608" alt="image" src="https://github.com/user-attachments/assets/43187002-0bcd-400c-80fb-5187b9736f4f" /> |
| 《FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts》| [link](https://arxiv.org/pdf/2311.05608) | FigStep is a black-box, no gradient needed jailbreaking algorithm against large vision-language models (VLMs). For instance, FigStep feeds harmful instructions into VLMs through the image channel and then uses benign text prompts to induce VLMs to output contents that violate common AI safety policies. Our experimental results reveal that VLMs are vulnerable to jailbreaking attacks, which highlights the necessity of novel safety alignments between visual and textual modalities. <br><br> ![image](https://github.com/user-attachments/assets/593b094e-e372-425f-8cf8-9ebc25004fc2) | 
| 《Derail Yourself: Multi-turn LLM Jailbreak Attack through Self-discovered Clues》| [link](https://arxiv.org/abs/2410.10700) | ![image](https://github.com/user-attachments/assets/6dc36be9-688b-4564-a8f3-e05f041f81a5) <br><br> ![image](https://github.com/user-attachments/assets/80c04574-2989-4464-9174-892223ccacd3) |
| 《Does Refusal Training in LLMs Generalize to the Past Tense?》| [link](https://arxiv.org/abs/2407.11969) | Refusal training in LLMs aims to prevent generating harmful or illegal content. This study reveals that rephrasing harmful queries in the past tense significantly weakens these guardrails. For example, "How to make a Molotov cocktail?" when reformulated to "How did people make a Molotov cocktail?" effectively bypasses the refusal mechanisms in various models, including GPT-4. ![image](https://github.com/user-attachments/assets/fc7d8ac7-2f03-44b7-a7e7-b2c5142ba5b7) |
| 《Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking》| [link](https://arxiv.org/abs/2311.09827) | While large language models (LLMs) have demonstrated increasing power, they have also called upon studies on their vulnerabilities. As representatives, jailbreak attacks can provoke harmful or unethical responses from LLMs, even after safety alignment. In this paper, we investigate a novel category of jailbreak attacks specifically designed to target the cognitive structure and processes of LLMs. Specifically, we analyze the safety vulnerability of LLMs in the face of 1) multilingual cognitive overload, 2) veiled expression, and 3) effect-to-cause reasoning. Different from previous jailbreak attacks, our proposed cognitive overload is a black-box attack with no need for knowledge of model architecture or access to model weights. Experiments conducted on AdvBench and MasterKey reveal that various LLMs, including both popular open-source model Llama 2 and the proprietary model ChatGPT, can be compromised through cognitive overload. Motivated by cognitive psychology work on managing cognitive load, we further investigate defending cognitive overload attack from two perspectives. Empirical studies show that our cognitive overload from three perspectives can jailbreak all studied LLMs successfully, while existing defense strategies can hardly mitigate the caused malicious uses effectively. <br><br> ![image](https://github.com/user-attachments/assets/ae93b39c-936d-4c94-ac17-a401342684d9) | 
| 《BEST-OF-N JAILBREAKING》| [link](https://arxiv.org/pdf/2412.03556) |  ![image](https://github.com/user-attachments/assets/62fd3190-f0d6-4d83-bebb-a3fc484ec29c) <br><br> ![image](https://github.com/user-attachments/assets/f904b6d5-bd3c-41cd-8d01-a5311878f5c0) |


 










