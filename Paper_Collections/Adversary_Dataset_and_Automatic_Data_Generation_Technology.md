## Papers

| Paper Name                                                       | Link                                     | Summary |
|------------------------------------------------------------------|------------------------------------------|----------|
| 《JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models》 | [link](https://arxiv.org/pdf/2404.01318) | The contributions of the JailbreakBench benchmark are as follows: <br><br> **(1) Repository of jailbreak artifacts**. We provide an evolving repository of artifacts corresponding to state-of-the-art jailbreaking attacks and defenses. Despite being critical for reproducible research, many such prompts have not been openly released, and crowd-sourced websites have gone offline (Albert, 2023). These artifacts can be accessed in the following way via our [library](https://github.com/JailbreakBench/jailbreakbench/). <br><br> **(2) Pipeline for red-teaming LLMs**. We provide a standardized pipeline for red-teaming LLMs. In particular, our pipeline implements the evaluation of potential jailbreaks, standardizes decoding parameters, and supports both local and cloud-based querying. <br><br> **(3) Pipeline for testing and adding new defenses**. We implement five baseline defenses which can be combined with any LLM. <br><br> **(4) Jailbreaking classifier selection**. Evaluating the success of jailbreak attacks is challenging given the subjective nature of judging the appropriateness of a LLM’s response. We perform a rigorous human evaluation to compare six jailbreak classifiers. Among these classifiers, we find the recent Llama-3-Instruct-70B to be an effective judge when used with a properly selected prompt. <br><br> **(5) Dataset of harmful and benign behaviors**. We introduce the JBB-Behaviors dataset, which comprises 100 distinct misuse behaviors divided into ten broad categories corresponding to OpenAI’s usage policies. Approximately half of these behaviors are original, while the other half are sourced from existing datasets (Zou et al., 2023; Mazeika et al., 2023, 2024). For each misuse behavior, we also collect a matching benign behavior on the same exact topic that can be used as a sanity check for evaluating refusal rates of new models and defenses. <br><br> **(6) Reproducible evaluation framework**. We provide a reproducible framework for evaluating the attack success rate of jailbreaking algorithms, which can also be used to submit an algorithm’s jailbreak strings to our artifact repository. <br><br> **(7) Jailbreaking leaderboard and website**. We maintain a website hosted at https://jailbreakbench.github.io/ which tracks the performance of jailbreaking attacks and defenses across various state-of-the-art LLMs on the official leaderboard. <br><br> <img width="944" alt="image" src="https://github.com/user-attachments/assets/e5e8a449-225b-4173-bb3c-061f52cc6603"> | 
| 《JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks》 | [link](https://arxiv.org/abs/2404.03027) | We introduce JailBreakV-28K, a comprehensive benchmark designed to evaluate the transferability of LLM jailbreak attacks to MLLMs, and further assess the robustness and safety of MLLMs against a variety of jailbreak attacks. We start by creating a comprehensive dataset that covers a wide range of malicious questions, i.e., our RedTeam-2K dataset, which is a collection of  2,000 malicious queries that span a broad spectrum of potential adversarial scenarios. Subsequently, based on the RedTeam-2K dataset, we generate 5,000 unique text-based jailbreak prompts using jailbreak techniques that work on LLMs Xu et al. (2024); Zeng et al. (2024); Zou et al. (2023); Liu et al. (2023b). To adapt these LLM jailbreak attacks for the multimodal context, we further pair these attacks with different types of images to produce 20,000 text-based LLM transfer jailbreak attacks, constituting part of our benchmark. Additionally, to make the proposed dataset more comprehensive, we also use recent image-based jailbreak attacks Gong et al. (2023); Liu et al. (2024b) and generate 8,000 additional jailbreak inputs. <br><br> ![image](https://github.com/user-attachments/assets/24134f3b-cc5d-415d-8d5b-37b7054c86ff) <br><br> The data generated from the above process constitute our comprehensive jailbreak dataset for MLLMs, the JailBreakV-28K, which contains 28,000 jailbreak test cases and covers a wide range of topics and attack strategies. <br><br> ![image](https://github.com/user-attachments/assets/81bf953b-4dc4-4701-9155-2ea3fcb54f95) |
| 《Trick Me If You Can: Human-in-the-loop Generation of Adversarial Examples for Question Answering》| [link](https://arxiv.org/pdf/1809.02701) | our human–computer hybrid approach uses human creativity to generate adversarial examples. A user interface presents model interpretations and helps users craft model-breaking examples. We apply this to a question answering (qa) task called Quizbowl, where trivia enthusiasts—who write questions for academic competitions—create diverse examples that stump existing qa models. <br><br> The user interface makes the adversarial writing process interactive and model-driven, in contrast to adversarial examples written independent of a model Ettinger et al. (2017). The result is an adversarially-authored dataset that explicitly exposes a model’s limitations by design. <br><br> ![image](https://github.com/user-attachments/assets/a73f9dc3-c1d1-48d2-9cf0-bcbed2574698 )<br><br> Human-in-the-loop generation can replace or aid model-based adversarial generation approaches. Creating interfaces and interpretations is often easier than designing and training generative models for specific domains. In domains where adversarial generation is feasible, human creativity can reveal which tactics automatic approaches can later emulate. Model-based and human-in-the-loop generation approaches can also be combined by training models to mimic human adversarial edit history, using the relative merits of both approaches.|
| 《TOXIGEN: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection》 | [link](https://arxiv.org/pdf/2404.01318) | In this work, we used a large language model to create and release ToxiGen, a large-scale, balanced, and implicit toxic language dataset. ToxiGen is far larger than previous datasets, containing over 274k sentences, and is more diverse, including mentions of 13 minority groups at scale. The generated samples are balanced in terms of number of benign and toxic samples for each group. We proposed Alice, an adversarial decoding scheme to evaluate robustness of toxicity classifiers and generate sentences to attack them, and showed the effectiveness of Alice on a number of publicly-available toxicity detection systems. In our experiments, we showed that fine-tuning pre-trained hate classifiers on ToxiGen can improve their performance on three popular human-generated toxicity datasets. We also conducted a human study on a subset of ToxiGen, verifying that our generation methods successfully create challenging statements that annotators struggle to distinguish from human-written text: 90.5% of machine-generated examples were thought to be human-written. <br><br> ![image](https://github.com/user-attachments/assets/5c8a15b5-636c-4a1b-8a7f-d8587c625604) | 
| 《Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs》| [link](https://arxiv.org/abs/2308.13387) | We introduced a comprehensive three-level taxonomy for assessing the risk of harms associated with LLMs, encompassing five distinct risk areas. Based on the taxonomy, we assembled a dataset consisting of 939 questions, alongside over 5,000 responses gathered from six different LLMs. We define the criteria of what is a safe and responsible answer to a risky question, and manually labeled all collected responses accordingly. <br><br> ![image](https://github.com/user-attachments/assets/ec67041c-6f84-4c95-8b74-7256581302a0) <br><br> Subsequently, we used these labeled responses to assess the safety mechanisms of the various LLMs. Furthermore, we explored novel methods to automatically appraise the safety mechanisms of these models using our dataset. Notably, our findings revealed that a suitably-trained small model (600M) can effectively perform the evaluation, yielding results that are comparable to those obtained using GPT-4 as an evaluator. ![image](https://github.com/user-attachments/assets/ccd3d740-c2dd-4b67-aaa3-111e24e3ca1e) |
| 《RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models》 | [link](https://arxiv.org/abs/2009.11462) | We introduce REALTOXICITYPROMPTS, a testbed of 100K prompts for evaluating the toxic degeneration in pretrained language models. <br><br> <img width="598" alt="image" src="https://github.com/user-attachments/assets/b6c967a4-1fa2-4afd-bf8c-ecb0a1add230"> <br><br> Under this framework, we quantify the toxicity of multiple pretrained language models and the effectiveness of methods for detoxifying generations. We then analyze toxicity in two large web text corpora, including the GPT-2 pretraining corpus, to better understand the root cause of toxic generations. Finally, we provide recommendations for gathering pretraining data. The data, code, and interactive visualizations for this paper can be found at https://toxicdegeneration.allenai.org/. |
| 《BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset》| [link](https://arxiv.org/abs/2307.04657) | In this paper, we introduce the BeaverTails dataset, aimed at fostering research on safety alignment in large language models (LLMs). This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes. In total, we have gathered safety meta-labels for 333,963 question-answer (QA) pairs and 361,903 pairs of expert comparison data for both the helpfulness and harmlessness metrics. <br><br> ![image](https://github.com/user-attachments/assets/f492eb1d-009a-4161-be24-ea99e796be4a) <br><br> We further showcase applications of BeaverTails in content moderation and reinforcement learning with human feedback (RLHF), emphasizing its potential for practical safety measures in LLMs. We believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of LLMs. Our project page is available at the following URL: https://sites.google.com/view/pku-beavertails. | 
| 《CValues: Measuring the Values of Chinese Large Language Models from Safety to Responsibility》 | [link](https://arxiv.org/abs/2307.09705) | With the rapid evolution of large language models (LLMs), there is a growing concern that they may pose risks or have negative social impacts. Therefore, evaluation of human values alignment is becoming increasingly important. Previous work mainly focuses on assessing the performance of LLMs on certain knowledge and reasoning abilities, while neglecting the alignment to human values, especially in a Chinese context. In this paper, we present CValues , the first Chinese human values evaluation benchmark to measure the alignment ability of LLMs in terms of both safety and responsibility criteria. <br><br> ![image](https://github.com/user-attachments/assets/f7f0f640-e5c2-48b7-9035-a71ebbfccd8c) <br><br> As a result, we have manually collected adversarial safety prompts across 10 scenarios and induced responsibility prompts from 8 domains by professional experts. To provide a comprehensive values evaluation of Chinese LLMs, we not only conduct human evaluation for reliable comparison, but also construct multi-choice prompts for automatic evaluation. Our findings suggest that while most Chinese LLMs perform well in terms of safety, there is considerable room for improvement in terms of responsibility. Moreover, both the automatic and human evaluation are important for assessing the human values alignment in different aspects. <br><br>![image](https://github.com/user-attachments/assets/abea6a83-6616-4d9e-946a-e850c57942c8) |
| 《Goal-Oriented Prompt Attack and Safety Evaluation for LLMs》 | [link](https://arxiv.org/abs/2309.11830) | In this paper, we introduce a pipeline to construct high-quality prompt attack samples, along with a Chinese prompt attack dataset called CPAD containing 10050 samples. <br><br> ![image](https://github.com/user-attachments/assets/68bca79f-bbd3-406c-bb28-88acd2c22273) <br><br> There are three key dimensions, content, template and goal from the perspective of attackers, which is different from previous studies. We utilize GPT-3.5 to extend manually-written seed samples, and only keep the successful prompts against three popular Chinese LLMs, where the evaluation prompts are constructed given the attacking goals and contents. <br><br> ![image](https://github.com/user-attachments/assets/4318b116-ddfe-4856-bbd3-85ef8fab850a) <br><br> We conduct analysis on responses from another four LLMs, including GPT-3.5. The evaluation shows that CPAD has an successful-attacking rate of around 70% against the LLMs. We also fine-tune Baichuan-13B-Chat using parts of CPAD, which improves the safety significantly. Our analysis reveals the weakness of LLMs including GPT-3.5, and indicates that there is still significant room for improvement in terms of safety. CPAD may contribute to further prompt attack studies. <br><br> ![image](https://github.com/user-attachments/assets/fbda9079-6c41-4b4d-acea-09f50bbd57d9) |
| 《JADE 3.0:大模型安全对齐》| [link](https://github.com/whitzard-ai/jade-db/tree/main/jade-db-v3.0) | https://mp.weixin.qq.com/s/_JiMAbqvXPsoS82Z2qD_vA |
| 《CTIBench: A Benchmark for Evaluating LLMs in Cyber Threat Intelligence》| [link](https://arxiv.org/abs/2406.07599) | Cyber threat intelligence (CTI) is crucial in today’s cybersecurity landscape, providing essential insights to understand and mitigate the ever-evolving cyber threats. The recent rise of Large Language Models (LLMs) have shown potential in this domain, but concerns about their reliability, accuracy, and hallucinations persist. While existing benchmarks provide general evaluations of LLMs, there are no benchmarks that address the practical and applied aspects of CTI-specific tasks. To bridge this gap, we introduce CTIBench, a benchmark designed to assess LLMs’ performance in CTI applications. CTIBench includes multiple datasets focused on evaluating knowledge acquired by LLMs in the cyber-threat landscape. Our evaluation of several state-of-the-art models on these tasks provides insights into their strengths and weaknesses in CTI contexts, contributing to a better understanding of LLM capabilities in CTI. <br><br> ![image](https://github.com/user-attachments/assets/70770d77-330e-4ab2-8f04-fae86af48e57) |
| 《SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems》| [link](https://w4ngatang.github.io/static/papers/superglue.pdf) | We present SuperGLUE, a new benchmark for evaluating general-purpose language understanding systems. SuperGLUE updates the GLUE benchmark by identifying a new set of challenging NLU tasks, as measured by the difference between human and machine baselines. The set of eight tasks in our benchmark emphasizes diverse task formats and low-data training data tasks, with nearly half the tasks having fewer than 1k examples and all but one of the tasks having fewer than 10k examples. We evaluate BERT-based baselines and find that they still lag behind humans by nearly 20 points. Given the difficulty of SuperGLUE for BERT, we expect that further progress in multi-task, transfer, and unsupervised/self-supervised learning techniques will be necessary to approach human-level performance on the benchmark. Overall, we argue that SuperGLUE offers a rich and challenging testbed for work developing new general-purpose machine learning methods for language understanding. <br><br> ![image](https://github.com/user-attachments/assets/609e568d-50bc-4c0b-a2d2-85650ac451df) |
| 《Agent-SafetyBench: Evaluating the Safety of LLM Agents》| [link](https://arxiv.org/abs/2412.14470) | https://github.com/thu-coai/Agent-SafetyBench | 



## Open-Source Resources
- [jailbreakbench](https://github.com/JailbreakBench/jailbreakbench)
  - [blog](https://safetyprompts.com/)
  - [huggingface](https://huggingface.co/datasets/JailbreakBench/JBB-Behaviors?row=0)
  - [news](https://mp.weixin.qq.com/s/YYAGC4dJTl60CT_uiPw4Hg)
- [JailBreakV_28K](https://github.com/EddyLuo1232/JailBreakV_28K)
- [TOXIGEN](https://github.com/microsoft/TOXIGEN)
- [pallms -- Payloads for Attacking Large Language Models](https://github.com/mik0w/pallms/tree/main)
- [Lakera/gandalf_ignore_instructions](https://huggingface.co/datasets/Lakera/gandalf_ignore_instructions)
- [JasperLS/prompt-injections](https://huggingface.co/datasets/JasperLS/prompt-injections)
- [pint-benchmark](https://github.com/lakeraai/pint-benchmark)
- [Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs](https://github.com/Libr-AI/do-not-answer/tree/main)
- [Real Toxicity Prompts](https://github.com/allenai/real-toxicity-prompts)
  - [huggingface](https://huggingface.co/datasets/allenai/real-toxicity-prompts)
  - [blog](https://toxicdegeneration.allenai.org/)
  - [paper](https://arxiv.org/abs/2009.11462)
- [BeaverTails: is a collection of datasets designed to facilitate research on safety alignment in large language models (LLMs)](https://github.com/PKU-Alignment/beavertails/tree/main)
  - [huggingface](https://huggingface.co/datasets/PKU-Alignment/BeaverTails?row=0)
  - [blog](https://sites.google.com/view/pku-beavertails)
  - [paper](https://arxiv.org/pdf/2307.04657)
- [SAP: the official repo of the paper "Attack Prompt Generation for Red Teaming and Defending Large Language Models" accepted to Findings of EMNLP 2023](https://github.com/Aatrox103/SAP/tree/main)
- [declare-lab/HarmfulQA](https://huggingface.co/datasets/declare-lab/HarmfulQA)
  - [red-instruct](https://github.com/declare-lab/red-instruct)
  - [declare-lab/starling-7B](https://huggingface.co/declare-lab/starling-7B)
- [CValues: 面向中文大模型价值观的评估与对齐研究](https://github.com/X-PLUG/CValues/tree/main)
- [CPAD: The official dataset of paper "Goal-Oriented Prompt Attack and Safety Evaluation for LLMs"](https://github.com/liuchengyuan123/CPAD)
- [ArtPrompt: Official Repo of ACL 2024 Paper `ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs`](https://github.com/uw-nsl/ArtPrompt/tree/main)
- [JADE 3.0：大模型安全对齐](https://github.com/whitzard-ai/jade-db/tree/main/jade-db-v3.0)
- [LLM evaluation datasets](https://huggingface.co/collections/clefourrier/llm-evaluation-datasets-64f9c369d3cd204ddde40ef8)
- [A collection of benchmarks and datasets for evaluating LLM](https://github.com/leobeeson/llm_benchmarks)
- [LLMs-Sec-Eval: 由亚信安全自建的数据集用于评估大模型在网络安全方向的专项能力](https://github.com/yaozhspider/LLMs-Sec-Eval/tree/main)
  - [news](https://mp.weixin.qq.com/s/wSBIakPOS2F6z4h5oVtgnw)
- [AlignBench](https://github.com/THUDM/AlignBench)
- [HarmBench Dataset](https://www.harmbench.org/explore)
  - [github](https://github.com/centerforaisafety/HarmBench)
  - [hugeface](https://huggingface.co/datasets/walledai/HarmBench/viewer/standard)
  - [hugeface](https://huggingface.co/datasets/walledai/TDC23-RedTeaming)
  - [hugeface](https://huggingface.co/datasets/coderchen01/HarmfulGeneration-HarmBench?row=0)
- [HarmfulQA](https://huggingface.co/datasets/declare-lab/CategoricalHarmfulQA?row=0)


