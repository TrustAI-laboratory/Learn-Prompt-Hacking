## Papers

| Paper Name                                                       | Link                                     | Summary |
|------------------------------------------------------------------|------------------------------------------|----------|
| 《JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models》 | [link](https://arxiv.org/pdf/2404.01318) | The contributions of the JailbreakBench benchmark are as follows: <br><br> **(1) Repository of jailbreak artifacts**. We provide an evolving repository of artifacts corresponding to state-of-the-art jailbreaking attacks and defenses. Despite being critical for reproducible research, many such prompts have not been openly released, and crowd-sourced websites have gone offline (Albert, 2023). These artifacts can be accessed in the following way via our [library](https://github.com/JailbreakBench/jailbreakbench/). <br><br> **(2) Pipeline for red-teaming LLMs**. We provide a standardized pipeline for red-teaming LLMs. In particular, our pipeline implements the evaluation of potential jailbreaks, standardizes decoding parameters, and supports both local and cloud-based querying. <br><br> **(3) Pipeline for testing and adding new defenses**. We implement five baseline defenses which can be combined with any LLM. <br><br> **(4) Jailbreaking classifier selection**. Evaluating the success of jailbreak attacks is challenging given the subjective nature of judging the appropriateness of a LLM’s response. We perform a rigorous human evaluation to compare six jailbreak classifiers. Among these classifiers, we find the recent Llama-3-Instruct-70B to be an effective judge when used with a properly selected prompt. <br><br> **(5) Dataset of harmful and benign behaviors**. We introduce the JBB-Behaviors dataset, which comprises 100 distinct misuse behaviors divided into ten broad categories corresponding to OpenAI’s usage policies. Approximately half of these behaviors are original, while the other half are sourced from existing datasets (Zou et al., 2023; Mazeika et al., 2023, 2024). For each misuse behavior, we also collect a matching benign behavior on the same exact topic that can be used as a sanity check for evaluating refusal rates of new models and defenses. <br><br> **(6) Reproducible evaluation framework**. We provide a reproducible framework for evaluating the attack success rate of jailbreaking algorithms, which can also be used to submit an algorithm’s jailbreak strings to our artifact repository. <br><br> **(7) Jailbreaking leaderboard and website**. We maintain a website hosted at https://jailbreakbench.github.io/ which tracks the performance of jailbreaking attacks and defenses across various state-of-the-art LLMs on the official leaderboard. <br><br> <img width="944" alt="image" src="https://github.com/user-attachments/assets/e5e8a449-225b-4173-bb3c-061f52cc6603"> | 
| 《JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks》 | [link](https://arxiv.org/abs/2404.03027) | We introduce JailBreakV-28K, a comprehensive benchmark designed to evaluate the transferability of LLM jailbreak attacks to MLLMs, and further assess the robustness and safety of MLLMs against a variety of jailbreak attacks. We start by creating a comprehensive dataset that covers a wide range of malicious questions, i.e., our RedTeam-2K dataset, which is a collection of  2,000 malicious queries that span a broad spectrum of potential adversarial scenarios. Subsequently, based on the RedTeam-2K dataset, we generate 5,000 unique text-based jailbreak prompts using jailbreak techniques that work on LLMs Xu et al. (2024); Zeng et al. (2024); Zou et al. (2023); Liu et al. (2023b). To adapt these LLM jailbreak attacks for the multimodal context, we further pair these attacks with different types of images to produce 20,000 text-based LLM transfer jailbreak attacks, constituting part of our benchmark. Additionally, to make the proposed dataset more comprehensive, we also use recent image-based jailbreak attacks Gong et al. (2023); Liu et al. (2024b) and generate 8,000 additional jailbreak inputs. <br><br> ![image](https://github.com/user-attachments/assets/24134f3b-cc5d-415d-8d5b-37b7054c86ff) <br><br> The data generated from the above process constitute our comprehensive jailbreak dataset for MLLMs, the JailBreakV-28K, which contains 28,000 jailbreak test cases and covers a wide range of topics and attack strategies. <br><br> ![image](https://github.com/user-attachments/assets/81bf953b-4dc4-4701-9155-2ea3fcb54f95) |
| 《Trick Me If You Can: Human-in-the-loop Generation of Adversarial Examples for Question Answering》| [link](https://arxiv.org/pdf/1809.02701) | our human–computer hybrid approach uses human creativity to generate adversarial examples. A user interface presents model interpretations and helps users craft model-breaking examples. We apply this to a question answering (qa) task called Quizbowl, where trivia enthusiasts—who write questions for academic competitions—create diverse examples that stump existing qa models. <br><br> The user interface makes the adversarial writing process interactive and model-driven, in contrast to adversarial examples written independent of a model Ettinger et al. (2017). The result is an adversarially-authored dataset that explicitly exposes a model’s limitations by design. <br><br> ![image](https://github.com/user-attachments/assets/a73f9dc3-c1d1-48d2-9cf0-bcbed2574698 )<br><br> Human-in-the-loop generation can replace or aid model-based adversarial generation approaches. Creating interfaces and interpretations is often easier than designing and training generative models for specific domains. In domains where adversarial generation is feasible, human creativity can reveal which tactics automatic approaches can later emulate. Model-based and human-in-the-loop generation approaches can also be combined by training models to mimic human adversarial edit history, using the relative merits of both approaches.|
| 《TOXIGEN: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection》 | [link](https://arxiv.org/pdf/2404.01318) | In this work, we used a large language model to create and release ToxiGen, a large-scale, balanced, and implicit toxic language dataset. ToxiGen is far larger than previous datasets, containing over 274k sentences, and is more diverse, including mentions of 13 minority groups at scale. The generated samples are balanced in terms of number of benign and toxic samples for each group. We proposed Alice, an adversarial decoding scheme to evaluate robustness of toxicity classifiers and generate sentences to attack them, and showed the effectiveness of Alice on a number of publicly-available toxicity detection systems. In our experiments, we showed that fine-tuning pre-trained hate classifiers on ToxiGen can improve their performance on three popular human-generated toxicity datasets. We also conducted a human study on a subset of ToxiGen, verifying that our generation methods successfully create challenging statements that annotators struggle to distinguish from human-written text: 90.5% of machine-generated examples were thought to be human-written. <br><br> ![image](https://github.com/user-attachments/assets/5c8a15b5-636c-4a1b-8a7f-d8587c625604) | 
| 《Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs》| [link](https://arxiv.org/abs/2308.13387) | We introduced a comprehensive three-level taxonomy for assessing the risk of harms associated with LLMs, encompassing five distinct risk areas. Based on the taxonomy, we assembled a dataset consisting of 939 questions, alongside over 5,000 responses gathered from six different LLMs. We define the criteria of what is a safe and responsible answer to a risky question, and manually labeled all collected responses accordingly. <br><br> ![image](https://github.com/user-attachments/assets/ec67041c-6f84-4c95-8b74-7256581302a0) <br><br> Subsequently, we used these labeled responses to assess the safety mechanisms of the various LLMs. Furthermore, we explored novel methods to automatically appraise the safety mechanisms of these models using our dataset. Notably, our findings revealed that a suitably-trained small model (600M) can effectively perform the evaluation, yielding results that are comparable to those obtained using GPT-4 as an evaluator. ![image](https://github.com/user-attachments/assets/ccd3d740-c2dd-4b67-aaa3-111e24e3ca1e) |
| 《RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models》 | [link](https://arxiv.org/abs/2009.11462) | We introduce REALTOXICITYPROMPTS, a testbed of 100K prompts for evaluating the toxic degeneration in pretrained language models. <br><br> <img width="598" alt="image" src="https://github.com/user-attachments/assets/b6c967a4-1fa2-4afd-bf8c-ecb0a1add230"> <br><br> Under this framework, we quantify the toxicity of multiple pretrained language models and the effectiveness of methods for detoxifying generations. We then analyze toxicity in two large web text corpora, including the GPT-2 pretraining corpus, to better understand the root cause of toxic generations. Finally, we provide recommendations for gathering pretraining data. The data, code, and interactive visualizations for this paper can be found at https://toxicdegeneration.allenai.org/. |
| 《BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset》| [link](https://arxiv.org/abs/2307.04657) | In this paper, we introduce the BeaverTails dataset, aimed at fostering research on safety alignment in large language models (LLMs). This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes. In total, we have gathered safety meta-labels for 333,963 question-answer (QA) pairs and 361,903 pairs of expert comparison data for both the helpfulness and harmlessness metrics. <br><br> ![image](https://github.com/user-attachments/assets/f492eb1d-009a-4161-be24-ea99e796be4a) <br><br> We further showcase applications of BeaverTails in content moderation and reinforcement learning with human feedback (RLHF), emphasizing its potential for practical safety measures in LLMs. We believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of LLMs. Our project page is available at the following URL: https://sites.google.com/view/pku-beavertails. | 



## Open-Source Resources
- [jailbreakbench](https://github.com/JailbreakBench/jailbreakbench)
- [JailBreakV_28K](https://github.com/EddyLuo1232/JailBreakV_28K)
- [TOXIGEN](https://github.com/microsoft/TOXIGEN)
- [pallms -- Payloads for Attacking Large Language Models](https://github.com/mik0w/pallms/tree/main)
- [Lakera/gandalf_ignore_instructions](https://huggingface.co/datasets/Lakera/gandalf_ignore_instructions)
- [JasperLS/prompt-injections](https://huggingface.co/datasets/JasperLS/prompt-injections)
- [pint-benchmark](https://github.com/lakeraai/pint-benchmark)
- [Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs](https://github.com/Libr-AI/do-not-answer/tree/main)
- [Real Toxicity Prompts](https://github.com/allenai/real-toxicity-prompts)
  - [huggingface](https://huggingface.co/datasets/allenai/real-toxicity-prompts)
  - [blog](https://toxicdegeneration.allenai.org/)
  - [paper](https://arxiv.org/abs/2009.11462)
- [BeaverTails: is a collection of datasets designed to facilitate research on safety alignment in large language models (LLMs)](https://github.com/PKU-Alignment/beavertails/tree/main)
  - [huggingface](https://huggingface.co/datasets/PKU-Alignment/BeaverTails?row=0)
  - [blog](https://sites.google.com/view/pku-beavertails)



