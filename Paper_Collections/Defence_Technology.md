| Paper Name                                                       | Link                                     | Summary |
|------------------------------------------------------------------|------------------------------------------|----------|
| 《GuardAgent: Safeguard LLM Agents by a Guard Agent via Knowledge-Enabled Reasoning》 | [link](https://arxiv.org/abs/2406.09187) | Existing methods for enhancing the safety of LLMs are not directly transferable to LLM-powered agents due to their diverse objectives and output modalities. In this paper, we propose GuardAgent, the first LLM agent as a guardrail to other LLM agents. Specifically, GuardAgent oversees a target LLM agent by checking whether its inputs/outputs satisfy a set of given guard requests (e.g., safety rules or privacy policies) defined by the users. GuardAgent comprises two steps: 1) creating a task plan by analyzing the provided guard requests, and 2) generating guardrail code based on the task plan and executing the code by calling APIs or using external engines. <br><br> ![image](https://github.com/user-attachments/assets/6dc67c33-e9e1-4639-98f9-69ee5c17bdcc) <br><br> In both steps, an LLM is utilized as the core reasoning component, supplemented by in-context demonstrations retrieved from a memory module. Such knowledge-enabled reasoning allows GuardAgent to understand various textual guard requests and accurately “translate” them into executable code that provides reliable guardrails. Furthermore, GuardAgent is equipped with an extendable toolbox containing functions and APIs and requires no additional LLM training, which underscores its generalization capabilities and low operational overhead. We also show that GuardAgent is able to define novel functions in adaption to emergent LLM agents and guard requests, which underscores its strong generalization capabilities.| 
| todo | link | todo |
