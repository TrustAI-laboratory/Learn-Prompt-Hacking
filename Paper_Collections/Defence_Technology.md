| Paper Name                                                       | Link                                     | Summary |
|------------------------------------------------------------------|------------------------------------------|----------|
| 《GuardAgent: Safeguard LLM Agents by a Guard Agent via Knowledge-Enabled Reasoning》 | [link](https://arxiv.org/abs/2406.09187) | Existing methods for enhancing the safety of LLMs are not directly transferable to LLM-powered agents due to their diverse objectives and output modalities. In this paper, we propose GuardAgent, the first LLM agent as a guardrail to other LLM agents. Specifically, GuardAgent oversees a target LLM agent by checking whether its inputs/outputs satisfy a set of given guard requests (e.g., safety rules or privacy policies) defined by the users. GuardAgent comprises two steps: 1) creating a task plan by analyzing the provided guard requests, and 2) generating guardrail code based on the task plan and executing the code by calling APIs or using external engines. <br><br> ![image](https://github.com/user-attachments/assets/6dc67c33-e9e1-4639-98f9-69ee5c17bdcc) <br><br> In both steps, an LLM is utilized as the core reasoning component, supplemented by in-context demonstrations retrieved from a memory module. Such knowledge-enabled reasoning allows GuardAgent to understand various textual guard requests and accurately “translate” them into executable code that provides reliable guardrails. Furthermore, GuardAgent is equipped with an extendable toolbox containing functions and APIs and requires no additional LLM training, which underscores its generalization capabilities and low operational overhead. We also show that GuardAgent is able to define novel functions in adaption to emergent LLM agents and guard requests, which underscores its strong generalization capabilities.| 
| 《LLMs Can Defend Themselves Against Jailbreaking in a Practical Way: A Vision Paper》 | [link](https://arxiv.org/abs/2402.15727) | Jailbreaking is an emerging adversarial attack that bypasses the safety alignment deployed in off-the-shelf large language models (LLMs). A considerable amount of research exists proposing more effective jailbreak attacks, including the recent Greedy Coordinate Gradient (GCG) attack, jailbreak template-based attacks such as using “Do-Anything-Now” (DAN), and multilingual jailbreak. In contrast, the defensive side has been relatively less explored. This paper proposes a lightweight yet practical defense called SelfDefend, which can defend against all existing jailbreak attacks with minimal delay for jailbreak prompts and negligible delay for normal user prompts. Our key insight is that regardless of the kind of jailbreak strategies employed, they eventually need to include a harmful prompt (e.g., “how to make a bomb”) in the prompt sent to LLMs, and we found that existing LLMs can effectively recognize such harmful prompts that violate their safety policies. Based on this insight, we design a shadow stack that concurrently checks whether a harmful prompt exists in the user prompt and triggers a checkpoint in the normal stack once a token of “No” or a harmful prompt is output. The latter could also generate an explainable LLM response to adversarial prompts. We demonstrate our idea of SelfDefend works in various jailbreak scenarios through manual analysis in GPT-3.5/4. We also list three future directions to further enhance SelfDefend. <br><br> ![image](https://github.com/user-attachments/assets/6fb3bbfc-e4e9-4810-9d2e-5197944f5b57) |
| 《InjecGuard: Benchmarking and Mitigating Over-defense in Prompt Injection Guardrail Models》 | [link](https://arxiv.org/pdf/2410.22770) | <img width="1043" alt="image" src="https://github.com/user-attachments/assets/c9f9c139-9c4b-4c18-a347-de570432c11b" /> | 

