| Paper Name                                                       | Link                                     | Summary |
|------------------------------------------------------------------|------------------------------------------|----------|
| 《Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey》 | [link](https://arxiv.org/abs/2402.09283) | Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject. <br><br> <img width="715" alt="image" src="https://github.com/user-attachments/assets/0bffea74-45b4-4629-9193-2fd9e7c04db0"> | 
| 《Baseline Defenses for Adversarial Attacks Against Aligned Language Models》 | [link](https://arxiv.org/abs/2309.00614) | As Large Language Models quickly become ubiquitous, it becomes critical to understand their security vulnerabilities. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision? <br><br> We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. We find that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs. Future research will be needed to uncover whether more powerful optimizers can be developed, or whether the strength of filtering and preprocessing defenses is greater in the LLMs domain than it has been in computer vision. |
| 《大模型安全漏洞报告——真实漏洞视角下的全面探讨》 | [link](https://pub1-bjyt.s3.360.cn/bcms/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E6%BC%8F%E6%B4%9E%E6%8A%A5%E5%91%8A.pdf) | 近年来，人工智能（AI）正以前所未有的速度发展，在各行各业中扮演着越来越重要的角色。大模型作为 AI 中的重要一环，其能力随着平台算力的提升、训练数据量的积累、深度学习算法的突破，得到进一步的提升，并逐渐在部分专业领域上崭露头角。与此同时，以大模型为核心涌现的大量技术应用，也在计算机安全领域带来了诸多新的风险和挑战。本文对大模型在软设施和具体应用场景落地中的安全问题进行多方面探讨和研究，涵盖了模型层安全、框架层安全、应用层安全。在研究过程中，我们借助 360 安全大模型代码分析能力，对多个开源项目进行代码梳理和风险评估，结合分析报告，快速审计并发现了近 40 个大模型相关安全漏洞，影响范围覆盖llama.cpp、Dify 等知名模型服务框架，以及 Intel 等国际厂商开发的多款开源产品。这些漏洞中，既存在二进制内存安全、Web 安全等经典漏洞类型，又包含由大模型自身特性引入的综合性问题。本文对不同场景下的攻击路径和可行性进行分析，并在文中结合了部分漏洞案例和具体说明，旨在从真实漏洞的视角下探索当前大模型的安全实践情况，为构建更加安全、健康的 AI 数字环境贡献力量。|
