| Paper Name                                                       | Link                                     | Summary |
|------------------------------------------------------------------|------------------------------------------|----------|
| 《JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models》 | [link](https://arxiv.org/pdf/2404.01318) | The contributions of the JailbreakBench benchmark are as follows: <br><br> **(1) Repository of jailbreak artifacts**. We provide an evolving repository of artifacts corresponding to state-of-the-art jailbreaking attacks and defenses. Despite being critical for reproducible research, many such prompts have not been openly released, and crowd-sourced websites have gone offline (Albert, 2023). These artifacts can be accessed in the following way via our [library](https://github.com/JailbreakBench/jailbreakbench/). <br><br> **(2) Pipeline for red-teaming LLMs**. We provide a standardized pipeline for red-teaming LLMs. In particular, our pipeline implements the evaluation of potential jailbreaks, standardizes decoding parameters, and supports both local and cloud-based querying. <br><br> **(3) Pipeline for testing and adding new defenses**. We implement five baseline defenses which can be combined with any LLM. <br><br> **(4) Jailbreaking classifier selection**. Evaluating the success of jailbreak attacks is challenging given the subjective nature of judging the appropriateness of a LLM’s response. We perform a rigorous human evaluation to compare six jailbreak classifiers. Among these classifiers, we find the recent Llama-3-Instruct-70B to be an effective judge when used with a properly selected prompt. <br><br> **(5) Dataset of harmful and benign behaviors**. We introduce the JBB-Behaviors dataset, which comprises 100 distinct misuse behaviors divided into ten broad categories corresponding to OpenAI’s usage policies. Approximately half of these behaviors are original, while the other half are sourced from existing datasets (Zou et al., 2023; Mazeika et al., 2023, 2024). For each misuse behavior, we also collect a matching benign behavior on the same exact topic that can be used as a sanity check for evaluating refusal rates of new models and defenses. <br><br> **(6) Reproducible evaluation framework**. We provide a reproducible framework for evaluating the attack success rate of jailbreaking algorithms, which can also be used to submit an algorithm’s jailbreak strings to our artifact repository. <br><br> **(7) Jailbreaking leaderboard and website**. We maintain a website hosted at https://jailbreakbench.github.io/ which tracks the performance of jailbreaking attacks and defenses across various state-of-the-art LLMs on the official leaderboard. <br><br> <img width="944" alt="image" src="https://github.com/user-attachments/assets/e5e8a449-225b-4173-bb3c-061f52cc6603"> | 
| 《JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks》 | [link](https://arxiv.org/abs/2404.03027) | We introduce JailBreakV-28K, a comprehensive benchmark designed to evaluate the transferability of LLM jailbreak attacks to MLLMs, and further assess the robustness and safety of MLLMs against a variety of jailbreak attacks. We start by creating a comprehensive dataset that covers a wide range of malicious questions, i.e., our RedTeam-2K dataset, which is a collection of  2,000 malicious queries that span a broad spectrum of potential adversarial scenarios. Subsequently, based on the RedTeam-2K dataset, we generate 5,000 unique text-based jailbreak prompts using jailbreak techniques that work on LLMs Xu et al. (2024); Zeng et al. (2024); Zou et al. (2023); Liu et al. (2023b). To adapt these LLM jailbreak attacks for the multimodal context, we further pair these attacks with different types of images to produce 20,000 text-based LLM transfer jailbreak attacks, constituting part of our benchmark. Additionally, to make the proposed dataset more comprehensive, we also use recent image-based jailbreak attacks Gong et al. (2023); Liu et al. (2024b) and generate 8,000 additional jailbreak inputs. <br><br> ![image](https://github.com/user-attachments/assets/24134f3b-cc5d-415d-8d5b-37b7054c86ff) <br><br> The data generated from the above process constitute our comprehensive jailbreak dataset for MLLMs, the JailBreakV-28K, which contains 28,000 jailbreak test cases and covers a wide range of topics and attack strategies. <br><br> ![image](https://github.com/user-attachments/assets/81bf953b-4dc4-4701-9155-2ea3fcb54f95) |
| 《Trick Me If You Can: Human-in-the-loop Generation of Adversarial Examples for Question Answering》| [link](https://arxiv.org/pdf/1809.02701) | our human–computer hybrid approach uses human creativity to generate adversarial examples. A user interface presents model interpretations and helps users craft model-breaking examples. We apply this to a question answering (qa) task called Quizbowl, where trivia enthusiasts—who write questions for academic competitions—create diverse examples that stump existing qa models. <br><br> The user interface makes the adversarial writing process interactive and model-driven, in contrast to adversarial examples written independent of a model Ettinger et al. (2017). The result is an adversarially-authored dataset that explicitly exposes a model’s limitations by design. <br><br> ![image](https://github.com/user-attachments/assets/a73f9dc3-c1d1-48d2-9cf0-bcbed2574698 )<br><br> Human-in-the-loop generation can replace or aid model-based adversarial generation approaches. Creating interfaces and interpretations is often easier than designing and training generative models for specific domains. In domains where adversarial generation is feasible, human creativity can reveal which tactics automatic approaches can later emulate. Model-based and human-in-the-loop generation approaches can also be combined by training models to mimic human adversarial edit history, using the relative merits of both approaches.|


Open-Source Resources
- [jailbreakbench](https://github.com/JailbreakBench/jailbreakbench)
- [JailBreakV_28K](https://github.com/EddyLuo1232/JailBreakV_28K)
