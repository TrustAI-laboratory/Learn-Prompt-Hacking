| Paper Name                                                       | Link                                     | Summary |
|------------------------------------------------------------------|------------------------------------------|----------|
| 《JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models》 | [link](https://arxiv.org/pdf/2404.01318) | The contributions of the JailbreakBench benchmark are as follows: <br><br> **(1) Repository of jailbreak artifacts**. We provide an evolving repository of artifacts corresponding to state-of-the-art jailbreaking attacks and defenses. Despite being critical for reproducible research, many such prompts have not been openly released, and crowd-sourced websites have gone offline (Albert, 2023). These artifacts can be accessed in the following way via our [library](https://github.com/JailbreakBench/jailbreakbench/). <br><br> **(2) Pipeline for red-teaming LLMs**. We provide a standardized pipeline for red-teaming LLMs. In particular, our pipeline implements the evaluation of potential jailbreaks, standardizes decoding parameters, and supports both local and cloud-based querying. <br><br> **(3) Pipeline for testing and adding new defenses**. We implement five baseline defenses which can be combined with any LLM. <br><br> **(4) Jailbreaking classifier selection**. Evaluating the success of jailbreak attacks is challenging given the subjective nature of judging the appropriateness of a LLM’s response. We perform a rigorous human evaluation to compare six jailbreak classifiers. Among these classifiers, we find the recent Llama-3-Instruct-70B to be an effective judge when used with a properly selected prompt. <br><br> **(5) Dataset of harmful and benign behaviors**. We introduce the JBB-Behaviors dataset, which comprises 100 distinct misuse behaviors divided into ten broad categories corresponding to OpenAI’s usage policies. Approximately half of these behaviors are original, while the other half are sourced from existing datasets (Zou et al., 2023; Mazeika et al., 2023, 2024). For each misuse behavior, we also collect a matching benign behavior on the same exact topic that can be used as a sanity check for evaluating refusal rates of new models and defenses. <br><br> **(6) Reproducible evaluation framework**. We provide a reproducible framework for evaluating the attack success rate of jailbreaking algorithms, which can also be used to submit an algorithm’s jailbreak strings to our artifact repository. <br><br> **(7) Jailbreaking leaderboard and website**. We maintain a website hosted at https://jailbreakbench.github.io/ which tracks the performance of jailbreaking attacks and defenses across various state-of-the-art LLMs on the official leaderboard. <br><br> <img width="944" alt="image" src="https://github.com/user-attachments/assets/e5e8a449-225b-4173-bb3c-061f52cc6603"> | 
| todo | link | todo |
