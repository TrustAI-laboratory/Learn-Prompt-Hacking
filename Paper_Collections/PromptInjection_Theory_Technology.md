| Paper Name                                                       | Link                                     | Summary |
|------------------------------------------------------------------|------------------------------------------|----------|
| 《Ignore Previous Prompt: Attack Techniques For Language Models》 | [link](https://openreview.net/pdf?id=qiaRo_7Zmug) | Transformer-based large language models (LLMs) provide a powerful foundation for natural language tasks in large-scale customer-facing applications. However, studies that explore their vulnerabilities emerging from malicious user interaction are scarce. By proposing PROMPTINJECT, a prosaic alignment framework for mask-based iterative adversarial prompt composition, we examine how GPT3, the most widely deployed language model in production, can be easily misaligned by simple handcrafted inputs. In particular, we investigate two types of attacks – goal hijacking and prompt leaking – and demonstrate that even lowaptitude, but sufficiently ill-intentioned agents, can easily exploit GPT-3’s stochastic nature, creating long-tail risks.  <br><br> <img width="986" alt="image" src="https://github.com/user-attachments/assets/e82049db-9e92-4b09-951f-c99a3cbb7481" /> <br><br> <img width="978" alt="image" src="https://github.com/user-attachments/assets/5d59424b-900e-42e7-bf8c-0fad7010437d" />| 
| 《Breaking ReAct Agents: Foot-in-the-Door Attack Will Get You In》| [link](https://arxiv.org/pdf/2410.16950) | <img width="1055" alt="image" src="https://github.com/user-attachments/assets/144d0ad6-fda9-44f7-a2af-b798e22c5ce1" /> | 
