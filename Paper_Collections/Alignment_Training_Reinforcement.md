| Paper Name                                                       | Link                                     | Summary |
|------------------------------------------------------------------|------------------------------------------|----------|
| 《The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions》 | [link](https://arxiv.org/pdf/2404.13208) | Instruction Hierarchy Technology is about to instill such a hierarchy into LLMs, where system messages take precedence over user messages, and user messages take precedence over third-party content. <br><br> ![image](https://github.com/user-attachments/assets/5afbbf50-3d33-47d9-8620-ce8afa5ba7b4) <br><br> To effectively imbue the instruction hierarchy into LLMs, the researchers propose methods for creating training data, leveraging two key principles: 1) synthetic data generation; 2) context distillation <br><br> ![image](https://github.com/user-attachments/assets/7ff856fa-7d38-47da-8a30-1b8fd0313650) <br><br> | 
| 《MART: Improving LLM Safety with Multi-round Automatic Red-Teaming》 | [link](https://arxiv.org/abs/2311.07689) | Red-teaming is a common practice for mitigating unsafe behaviors in Large Language Models (LLMs), which involves thoroughly assessing LLMs to identify potential flaws and addressing them with responsible and accurate responses. While effective, manual red-teaming is costly, and existing automatic red-teaming typically discovers safety risks without addressing them. In this paper, we propose a Multi-round Automatic Red-Teaming (Mart) method, which incorporates both automatic adversarial prompt writing and safe response generation, significantly increasing red-teaming scalability and the safety of the target LLM. Specifically, an adversarial LLM and a target LLM interplay with each other in an iterative manner, where the adversarial LLM aims to generate challenging prompts that elicit unsafe responses from the target LLM, while the target LLM is fine-tuned with safety aligned data on these adversarial prompts. In each round, the adversarial LLM crafts better attacks on the updated target LLM, while the target LLM also improves itself through safety fine-tuning. On adversarial prompt benchmarks, the violation rate of an LLM with limited safety alignment reduces up to 84.7% after 4 rounds of Mart, achieving comparable performance to LLMs with extensive adversarial prompt writing. Notably, model helpfulness on non-adversarial prompts remain stable throughout iterations, indicating the target LLM maintains strong performance on instruction following. <br><br> ![image](https://github.com/user-attachments/assets/66211aec-730d-4eed-9dbd-7e399496a961) |
| 《Recipes for Safety in Open-domain Chatbots》| [link](https://arxiv.org/abs/2010.07079) | Models trained on large unlabeled corpora of human interactions will learn patterns and mimic behaviors therein, which include offensive or otherwise toxic behavior and unwanted biases. We investigate a variety of methods to mitigate these issues in the context of open-domain generative dialogue models. We introduce a new human-and-model-in-the-loop framework for both training safer models and for evaluating them, as well as a novel method to distill safety considerations inside generative models without the use of an external classifier at deployment time. We conduct experiments comparing these methods and find our new techniques are (i) safer than existing models as measured by automatic and human evaluations while (ii) maintaining usability metrics such as engagingness relative to the state of the art. We then discuss the limitations of this work by analyzing failure cases of our models. <br><br> ![image](https://github.com/user-attachments/assets/437cc7c9-2a1e-458c-b8fc-c31f900b596a) |
| 《Deliberative Alignment: Reasoning Enables Safer Language Models》 | [link](https://www.arxiv.org/abs/2412.16339) | <img width="865" alt="image" src="https://github.com/user-attachments/assets/680da64e-e267-4fe8-8970-982fd631739d" /> |
| 《StruQ: Defending Against Prompt Injection with Structured Queries》| [link](https://arxiv.org/pdf/2402.06363) | <img width="1004" alt="image" src="https://github.com/user-attachments/assets/8cb7e09b-1dad-48c7-a080-25e650b15570" /> | 
| 《Aligning LLMs to Be Robust Against Prompt Injection》 | [link](https://arxiv.org/pdf/2410.05451) | <img width="1171" alt="image" src="https://github.com/user-attachments/assets/2c1904ca-fbd8-4b08-968e-2a14e5c464a2" /> |
| 《Detoxifying Large Language Models via Knowledge Editing》| [link](https://arxiv.org/pdf/2403.14472) | <img width="537" alt="image" src="https://github.com/user-attachments/assets/d2b7a2d1-3ba1-4e45-b418-a0e567a56cc9" /> | 
| 《What Makes and Breaks Safety Fine-tuning? A Mechanistic Study》| [link](https://arxiv.org/pdf/2407.10264) | We proposed a synthetic data generation framework to systematically and efficiently analyze safety fine-tuning methods and craft jailbreak attacks. Using this, we found that safety fine-tuning encourages formation of different clusters for safe and unsafe samples while making the model significantly less sensitive towards unsafe samples. We also observed that samples for jailbreak and adversarial attacks are more similar to safe samples than they are to unsafe ones, hence bypassing the safety mechanism learned by the model and avoid a refusal. Though we primarily focus on a synthetic, but well grounded, abstraction of real language data, several of our claims directly transfer to more realistic setups, as shown by our experiments on Llama models. Broadly, then, our results echo the claims in recent work that state safety fine-tuning minimally alters a model (Kotha et al., 2023; Prakash et al., 2024; Qi et al., 2023; Lee et al., 2024; Jain et al., 2023b; Lubana et al., 2022), highlighting a need for rethinking the pipeline for safety and alignment inducing protocols. |

